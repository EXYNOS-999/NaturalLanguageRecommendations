{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "build_index_and_search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/NaturalLanguageRecommendations/blob/master/notebooks/inference/build_index_and_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf93M9RFhUj2",
        "colab_type": "text"
      },
      "source": [
        "### Works Best With TPU Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8wAw2JV6F4Am",
        "outputId": "cf256147-6101-4729-e68a-1aa1a1cb026a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "!gdown --id \"10LV9QbZOkUyOzR4nh8hxesoKJhpmvpM9\"   # citation vectors\n",
        "!gdown --id \"1-23aNm7j0bnycvyd_OaQfofVYPTewgOI\"   # abstract vectors\n",
        "!gdown --id \"1NyUQwgUNj9bFsiCnZ2TfKmWn5r-Y6wav\"   # TitlesIdAbstractsEmbedIds\n",
        "!wget 'https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar'\n",
        "!tar -xvf 'scibert_scivocab_uncased.tar'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10LV9QbZOkUyOzR4nh8hxesoKJhpmvpM9\n",
            "To: /content/CitationSimilarityVectors106Epochs.npy\n",
            "2.59GB [00:23, 111MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-23aNm7j0bnycvyd_OaQfofVYPTewgOI\n",
            "To: /content/AbstractSimVectors.npy\n",
            "2.59GB [00:40, 64.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NyUQwgUNj9bFsiCnZ2TfKmWn5r-Y6wav\n",
            "To: /content/TitlesIdsAbstractsEmbedIdsCOMPLETE_12-30-19.json.gzip\n",
            "432MB [00:04, 94.3MB/s]\n",
            "--2019-12-31 15:19:11--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.237.64\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.237.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 442460160 (422M) [application/x-tar]\n",
            "Saving to: ‘scibert_scivocab_uncased.tar’\n",
            "\n",
            "scibert_scivocab_un 100%[===================>] 421.96M  53.1MB/s    in 8.1s    \n",
            "\n",
            "2019-12-31 15:19:20 (51.9 MB/s) - ‘scibert_scivocab_uncased.tar’ saved [442460160/442460160]\n",
            "\n",
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/vocab.txt\n",
            "scibert_scivocab_uncased/pytorch_model.bin\n",
            "scibert_scivocab_uncased/config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP2AqdEiQhbS",
        "colab_type": "code",
        "outputId": "e9a11d0b-aa73-4153-e5e5-d2e47bb934fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 450kB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 860kB 58.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 37.2MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SGov6e8uGO3B",
        "outputId": "011c5e3f-f0f6-4520-b3f2-ca9751fac6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "print('TensorFlow:', tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "TensorFlow: 2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iht3WQa3H_g6",
        "outputId": "883afc45-e835-4f75-bdea-3dbd2ace9b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.74.246.146:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: 10.74.246.146:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.74.246.146:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ks95sxuIKbR",
        "outputId": "6da7b0dc-7895-4980-abdc-67aedddf9c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "if tpu:\n",
        "  workers = ['/job:worker/replica:0/task:0/device:TPU:'+str(i) for i in range(8)]\n",
        "  print(workers)\n",
        "  gpu =''\n",
        "if tf.config.experimental.list_physical_devices('GPU'):\n",
        "  gpu = 'gpu'\n",
        "  workers = ['/GPU:0']\n",
        "  pprint(workers)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/job:worker/replica:0/task:0/device:TPU:0', '/job:worker/replica:0/task:0/device:TPU:1', '/job:worker/replica:0/task:0/device:TPU:2', '/job:worker/replica:0/task:0/device:TPU:3', '/job:worker/replica:0/task:0/device:TPU:4', '/job:worker/replica:0/task:0/device:TPU:5', '/job:worker/replica:0/task:0/device:TPU:6', '/job:worker/replica:0/task:0/device:TPU:7']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uByeXRrVO78_",
        "colab": {}
      },
      "source": [
        "class Index:\n",
        "    def __init__(self, embeddings, worker):\n",
        "        self.embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
        "        self.worker = worker\n",
        "\n",
        "    @tf.function\n",
        "    def search(self, query_vector):\n",
        "      with tf.device(worker):\n",
        "        dot_product = tf.reduce_sum(tf.multiply(self.embeddings, query_vector), axis=1)\n",
        "        distances = 1 - dot_product\n",
        "        sorted_indices =  tf.argsort(distances)\n",
        "        nearest_distances = tf.gather(distances, sorted_indices)\n",
        "        return nearest_distances[:20], sorted_indices[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFloyWt7G8ts",
        "colab": {}
      },
      "source": [
        "citations_embeddings = np.load('CitationSimilarityVectors106Epochs.npy')\n",
        "abstract_embeddings = np.load('AbstractSimVectors.npy')\n",
        "assert citations_embeddings.shape == abstract_embeddings.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AB8L4sA-H2nf",
        "outputId": "449d3afb-6871-4f67-9c7f-93e3cac9be92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if tpu:\n",
        "  # Discarding last 4 vectors to make number of vectors divisible by 8\n",
        "  citations_embeddings = np.split(citations_embeddings[:-4], 8, axis=0)\n",
        "  abstract_embeddings = np.split(abstract_embeddings[:-4], 8, axis=0)\n",
        "  vecs_per_index = citations_embeddings[0].shape[0]\n",
        "  print('Vectors per index :', vecs_per_index)\n",
        "elif gpu:\n",
        "  vecs_per_index = citations_embeddings.shape[0]\n",
        "  print('Vectors per index :', vecs_per_index)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors per index : 157874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HoEA0SqeJW4n",
        "outputId": "6f19c006-7f78-4119-dcc4-d31eea17caeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "citation_indices = []\n",
        "abstract_indices = []\n",
        "if gpu:\n",
        "  for i, worker in enumerate(workers):\n",
        "    with tf.device(worker):\n",
        "      print('Building index with {} vectors on {}'.format(citations_embeddings.shape[0], worker))\n",
        "      citation_indices.append(Index(citations_embeddings, worker))\n",
        "      abstract_indices.append(Index(abstract_embeddings, worker))\n",
        "elif tpu:\n",
        "  ## Place 1/8 of total embeddings on each TPU core\n",
        "  for i, worker in enumerate(workers):\n",
        "    with tf.device(worker):\n",
        "      print('Building index with {} vectors on {}'.format(citations_embeddings[i].shape[0],worker))\n",
        "      citation_indices.append(Index(citations_embeddings[i], worker))\n",
        "      abstract_indices.append(Index(abstract_embeddings[i], worker))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building index with 157874 vectors on /job:worker/replica:0/task:0/device:TPU:0\n",
            "Building index with 157874 vectors on /job:worker/replica:0/task:0/device:TPU:1\n",
            "Building index with 157874 vectors on /job:worker/replica:0/task:0/device:TPU:2\n",
            "Building index with 157874 vectors on /job:worker/replica:0/task:0/device:TPU:3\n",
            "Building index with 157874 vectors on /job:worker/replica:0/task:0/device:TPU:4\n",
            "Building index with 157874 vectors on /job:worker/replica:0/task:0/device:TPU:5\n",
            "Building index with 157874 vectors on /job:worker/replica:0/task:0/device:TPU:6\n",
            "Building index with 157874 vectors on /job:worker/replica:0/task:0/device:TPU:7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W5r4LmcVqGAZ",
        "colab": {}
      },
      "source": [
        "def search(xq, top_k=10):\n",
        "  cD, cI = [], []\n",
        "  aD, aI = [], []\n",
        "  if tpu:\n",
        "    r = 8\n",
        "  else:\n",
        "    r = 1\n",
        "  for i in range(r):\n",
        "    print('Search running on {}'.format(citation_indices[i].worker))\n",
        "    cd, cidx = citation_indices[i].search(xq)\n",
        "    ad, aidx = abstract_indices[i].search(xq)\n",
        "\n",
        "    cD.extend(cd.numpy())\n",
        "    aD.extend(ad.numpy())\n",
        "\n",
        "    cI.extend(i*vecs_per_index + cidx.numpy())\n",
        "    aI.extend(i*vecs_per_index + aidx.numpy())\n",
        "\n",
        "  cid_sorted = np.argsort(cD)[:top_k]\n",
        "  aid_sorted = np.argsort(aD)[:top_k]\n",
        "\n",
        "  cD = np.array(cD)[cid_sorted]\n",
        "  aD = np.array(aD)[aid_sorted]\n",
        "\n",
        "  cI = np.array(cI)[cid_sorted]\n",
        "  aI = np.array(aI)[aid_sorted]\n",
        "  return cD, aD, cI, aI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syc0k8Yi4rcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.saved_model.load('gs://tfworld/saved_models')\n",
        "tokenizer = BertTokenizer(vocab_file='scibert_scivocab_uncased/vocab.txt')\n",
        "\n",
        "df = pd.read_json('/content/TitlesIdsAbstractsEmbedIdsCOMPLETE_12-30-19.json.gzip', compression = 'gzip')\n",
        "embed2Title = pd.Series(df['title'].values,index=df['EmbeddingID']).to_dict()\n",
        "embed2Abstract = pd.Series(df['paperAbstract'].values,index=df['EmbeddingID']).to_dict()\n",
        "embed2Paper = pd.Series(df['id'].values,index=df['EmbeddingID']).to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxkcuP0CDNKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embed_id = 70000\n",
        "# title = embed2Title[embed_id]\n",
        "#abstract = embed2Abstract[embed_id]\n",
        "# abstract = '''Forecasting stock market direction is always an amazing but challenging problem in finance. Although many popular shallow computational methods (such as Backpropagation Network and Support Vector Machine) have extensively been proposed, most algorithms have not yet attained a desirable level of applicability. In this paper, we present a deep learning model with strong ability to generate high level feature representations for accurate financial prediction. Precisely, a stacked denoising autoencoder (SDAE) from deep learning is applied to predict the daily CSI 300 index, from Shanghai and Shenzhen Stock Exchanges in China. We use six evaluation criteria to evaluate its performance compared with the back propagation network, support vector machine. The experiment shows that the underlying financial model with deep machine technology has a significant advantage for the prediction of the CSI 300 index.'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFopFGOqit5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abstract = \"The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJJnXM5BiE-m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "00cdaeee-3db0-4471-8d5f-5232ca8def23"
      },
      "source": [
        "abstract_encoded = tokenizer.encode(abstract, max_length=512, pad_to_max_length=True)\n",
        "abstract_encoded = tf.constant(abstract_encoded, dtype=tf.int32)[None, :]\n",
        "print('\\nAbstract : ')\n",
        "pprint(abstract)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Abstract : \n",
            "('The highest accuracy object detectors to date are based on a two-stage '\n",
            " 'approach popularized by R-CNN, where a classifier is applied to a sparse set '\n",
            " 'of candidate object locations. In contrast, one-stage detectors that are '\n",
            " 'applied over a regular, dense sampling of possible object locations have the '\n",
            " 'potential to be faster and simpler, but have trailed the accuracy of '\n",
            " 'two-stage detectors thus far. In this paper, we investigate why this is the '\n",
            " 'case. We discover that the extreme foreground-background class imbalance '\n",
            " 'encountered during training of dense detectors is the central cause. We '\n",
            " 'propose to address this class imbalance by reshaping the standard cross '\n",
            " 'entropy loss such that it down-weights the loss assigned to well-classified '\n",
            " 'examples. Our novel Focal Loss focuses training on a sparse set of hard '\n",
            " 'examples and prevents the vast number of easy negatives from overwhelming '\n",
            " 'the detector during training. To evaluate the effectiveness of our loss, we '\n",
            " 'design and train a simple dense detector we call RetinaNet. Our results show '\n",
            " 'that when trained with the focal loss, RetinaNet is able to match the speed '\n",
            " 'of previous one-stage detectors while surpassing the accuracy of all '\n",
            " 'existing state-of-the-art two-stage detectors.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-_vAr8z8Hh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = time()\n",
        "bert_output = model(abstract_encoded)\n",
        "xq = tf.nn.l2_normalize(bert_output, axis=1)\n",
        "prediction_time = time() - s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhiyvUxrF4xr",
        "colab_type": "code",
        "outputId": "8e65cfa0-3ecb-4aba-a9f1-91a953aee629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "s_s = time()\n",
        "cD, aD, cI, aI = search(xq, top_k=5)\n",
        "search_time = time() - s_s\n",
        "total_time = prediction_time + search_time\n",
        "print('\\n'*2)\n",
        "print('Prediction time  :', np.round(prediction_time, 3), 'secs')\n",
        "print('Search time      :', np.round(search_time, 3), 'secs')\n",
        "print('Total time       :', np.round(total_time, 3), 'secs')\n",
        "\n",
        "print('\\n'*2)\n",
        "print('*'*80)\n",
        "for i in range(len(cI)):\n",
        "  print('Title : ')\n",
        "  pprint(embed2Title[cI[i]])\n",
        "  print('\\n')\n",
        "  pprint('Link: semanticscholar.org/paper/'+embed2Paper[cI[i]])\n",
        "  print('*'*80, )\n",
        "print('\\nNeighbours       :', cI )\n",
        "print('Distances        :', np.round(cD, 4))\n",
        "\n",
        "print('\\n'*4)\n",
        "print('*'*80)\n",
        "for i in range(len(aI)):\n",
        "  print('Abstract : ')\n",
        "  pprint(embed2Abstract[aI[i]])\n",
        "  print('\\n')\n",
        "  pprint('Link: semanticscholar.org/paper/'+embed2Paper[aI[i]])\n",
        "  print('*'*80)\n",
        "print('\\nNeighbours       :', aI )\n",
        "print('Distances        :', np.round(aD, 4))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search running on /job:worker/replica:0/task:0/device:TPU:0\n",
            "Search running on /job:worker/replica:0/task:0/device:TPU:1\n",
            "Search running on /job:worker/replica:0/task:0/device:TPU:2\n",
            "Search running on /job:worker/replica:0/task:0/device:TPU:3\n",
            "Search running on /job:worker/replica:0/task:0/device:TPU:4\n",
            "Search running on /job:worker/replica:0/task:0/device:TPU:5\n",
            "Search running on /job:worker/replica:0/task:0/device:TPU:6\n",
            "Search running on /job:worker/replica:0/task:0/device:TPU:7\n",
            "\n",
            "\n",
            "\n",
            "Prediction time  : 0.002 secs\n",
            "Search time      : 0.394 secs\n",
            "Total time       : 0.396 secs\n",
            "\n",
            "\n",
            "\n",
            "********************************************************************************\n",
            "Title : \n",
            "'SpreadOut: A Kernel Weight Initializer for Convolutional Neural Networks'\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/3839080f524090e63063e94709f51d7036663489'\n",
            "********************************************************************************\n",
            "Title : \n",
            "'Improving multi-label classification using scene cues'\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/f0a8909fd16010cb662806dffc987e266c072a54'\n",
            "********************************************************************************\n",
            "Title : \n",
            "('SqueezeMap: Fast Pedestrian Detection on a Low-Power Automotive Processor '\n",
            " 'Using Efficient Convolutional Neural Networks')\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/85df1c2ed78788699f805b43a40f041813f04f2e'\n",
            "********************************************************************************\n",
            "Title : \n",
            "('A Deep Fully Convolution Neural Network for Semantic Segmentation Based on '\n",
            " 'Adaptive Feature Fusion')\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/4ab2aa7381e11c7816f0330ad4f61cadd210533e'\n",
            "********************************************************************************\n",
            "Title : \n",
            "'Image Target Detection Based on Deep Convolutional Neural Network'\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/32de0d6f45c4d5ef46a8a1f0d3c9ca38bd9dc481'\n",
            "********************************************************************************\n",
            "\n",
            "Neighbours       : [  50520  225124  131315 1223630  987811]\n",
            "Distances        : [0.3649 0.3705 0.3721 0.375  0.3771]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********************************************************************************\n",
            "Abstract : \n",
            "('The highest accuracy object detectors to date are based on a two-stage '\n",
            " 'approach popularized by R-CNN, where a classifier is applied to a sparse set '\n",
            " 'of candidate object locations. In contrast, one-stage detectors that are '\n",
            " 'applied over a regular, dense sampling of possible object locations have the '\n",
            " 'potential to be faster and simpler, but have trailed the accuracy of '\n",
            " 'two-stage detectors thus far. In this paper, we investigate why this is the '\n",
            " 'case. We discover that the extreme foreground-background class imbalance '\n",
            " 'encountered during training of dense detectors is the central cause. We '\n",
            " 'propose to address this class imbalance by reshaping the standard cross '\n",
            " 'entropy loss such that it down-weights the loss assigned to well-classified '\n",
            " 'examples. Our novel Focal Loss focuses training on a sparse set of hard '\n",
            " 'examples and prevents the vast number of easy negatives from overwhelming '\n",
            " 'the detector during training. To evaluate the effectiveness of our loss, we '\n",
            " 'design and train a simple dense detector we call RetinaNet. Our results show '\n",
            " 'that when trained with the focal loss, RetinaNet is able to match the speed '\n",
            " 'of previous one-stage detectors while surpassing the accuracy of all '\n",
            " 'existing state-of-the-art two-stage detectors.')\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/79cfb51a51fc093f66aac8e858afe2e14d4a1f20'\n",
            "********************************************************************************\n",
            "Abstract : \n",
            "('Object detector with region proposal networks such as Fast/Faster R-CNN [1, '\n",
            " '2] have shown the state-of-the art performance on several benchmarks. '\n",
            " 'However, they have limited success for detecting small objects. We argue the '\n",
            " 'limitation is related to insufficient performance of Fast R-CNN block in '\n",
            " 'Faster R-CNN. In this paper, we propose a refining block for Fast R-CNN. We '\n",
            " 'further merge the block and Faster R-CNN into a single network (RF-RCNN). '\n",
            " 'The RF-RCNN was applied on plate and human detection in RoadView image that '\n",
            " 'consists of high resolution street images (over 30M pixels). As a result, '\n",
            " 'the RF-RCNN showed great improvement over the Faster-RCNN.')\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/233b52f79d37363d924041e047c6b9d9609cbb17'\n",
            "********************************************************************************\n",
            "Abstract : \n",
            "('This paper presents a novel multi-modal CNN architecture for object '\n",
            " 'detection by exploiting complementary input cues in addition to sole color '\n",
            " 'information. Our one-stage architecture fuses the multiscale mid-level '\n",
            " 'features from two individual feature extractor, so that our end-to-end net '\n",
            " 'can accept cross modal streams to obtain high-precision detection results. '\n",
            " 'In comparison to other cross modal fusion neural networks, our solution '\n",
            " 'successfully reduces runtime to meet the real-time requirement with still '\n",
            " 'high-level accuracy. Experimental evaluation on challenging NYUD2 dataset '\n",
            " 'shows that our network achieves 49.1% mAP, and processes images in real-time '\n",
            " 'at 35.3 frames per second on one single Nvidia GTX1080 GPU. Compared to '\n",
            " 'baseline one stage network SSD on RGB images which gets 39.2% mAP, our '\n",
            " 'method has great accuracy improvement.')\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/f977591a9ef50df383b061347a5434033a7f5795'\n",
            "********************************************************************************\n",
            "Abstract : \n",
            "('In this paper, we discuss the relevance of training data on modern object '\n",
            " 'detectors used on onboard applications. Whereas modern deep learning '\n",
            " 'techniques require large amounts of data, datasets with typical scenarios '\n",
            " 'for autonomous vehicles are scarce and have a reduced number of samples. We '\n",
            " 'conduct a comprehensive set of experiments to understand the effect of using '\n",
            " 'a combination of two relatively small datasets to train an end-to-end object '\n",
            " 'detector, based on the popular Faster R-CNN and enhanced with orientation '\n",
            " 'estimation capabilities. We also test the adequacy of training models using '\n",
            " 'partially available ground-truth labels, as a consequence of combining '\n",
            " 'datasets aimed at different applications. Data augmentation is also '\n",
            " 'introduced into the training pipeline. Results show a significant '\n",
            " 'performance improvement in our exemplary case as a result of the higher '\n",
            " 'variability of the training samples, thus opening a new way to improve the '\n",
            " 'detection performance independently from the detector architecture.')\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/3ef85a078a688615c61f04d8605ae27e17f554dd'\n",
            "********************************************************************************\n",
            "Abstract : \n",
            "('In this paper, we propose a novel deep convolutional network for object '\n",
            " 'detection named densely convolutional and feature fused object '\n",
            " 'detector(DCFF-Net), which is a one-stage object detector from scratch '\n",
            " 'similarly to DSOD. The base network is stacking by several densely '\n",
            " 'convolutional blocks to extract the powerful semantic information, and the '\n",
            " 'feature fusion module is used to obtain the enriching features by fusing the '\n",
            " 'extracted feature maps from different convolutional layers. In the fusion '\n",
            " 'module, the feature maps are concatenated of three adjacent scales, which '\n",
            " 'are from the features extracted by the convolution with big kernels, the '\n",
            " 'features extracted by down-sampling pooling and the features extracted by '\n",
            " 'up-sampling deconvolution. The fused feature pyramid has more representative '\n",
            " 'information and gets better performances when it is fed to the final '\n",
            " 'multibox detectors. On the Pascal VOC 2007/2012 and MS COCO, our network '\n",
            " 'achieves better results than DSOD and several methods with pre-training '\n",
            " 'models. The experimental results show that our proposed network has better '\n",
            " 'detection performance by the aid of the fusion of different layers’ feature '\n",
            " 'maps, especially on small objects and occluded objects.')\n",
            "\n",
            "\n",
            "'Link: semanticscholar.org/paper/33a82746204a2788b210b0694a56188e54ee2732'\n",
            "********************************************************************************\n",
            "\n",
            "Neighbours       : [ 720951 1102463  182199  463456  989858]\n",
            "Distances        : [-0.      0.0527  0.063   0.0638  0.0647]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmFJS0xIc9us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}