{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoNaturalLanguageRecommendationsSimpleDemoCPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/NaturalLanguageRecommendations/blob/master/notebooks/inference/DemoNaturalLanguageRecommendationsSimpleDemoCPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9eERu8JP9r7",
        "colab_type": "text"
      },
      "source": [
        "This is our simple Colab demo notebook, which can run on a CPU instance, though it may crash a regular colab CPU instance since it's memory intensive. If that happens, a message should appear on the bottom left asking if you would like to switch to a 25 gb RAM instance, which will be more than enough memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3YuDoc3QUBv",
        "colab_type": "text"
      },
      "source": [
        "If you would like play with using a GPU or TPU for inference, please see our advanced demo notebook here https://colab.research.google.com/github/Santosh-Gupta/NaturalLanguageRecommendations/blob/master/notebooks/inference/build_index_and_search.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8wAw2JV6F4Am",
        "outputId": "a5d7f8db-8120-4d4c-98cb-1eecb40dfead",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "#@title Download and load model, embeddings, and data, will take a several minutes. Double click on this to pop open the hood and checkout the code.\n",
        "\n",
        "!gdown --id \"10LV9QbZOkUyOzR4nh8hxesoKJhpmvpM9\"   # citation vectors\n",
        "# !gdown --id \"1-8gmT9cQpOUoZ_HzEaT9Xz6qfeVooAFn\"\n",
        "!gdown --id \"1-23aNm7j0bnycvyd_OaQfofVYPTewgOI\"   # abstract vectors\n",
        "!gdown --id \"1NyUQwgUNj9bFsiCnZ2TfKmWn5r-Y6wav\"   # TitlesIdAbstractsEmbedIds\n",
        "!gdown --id \"1wIRsAApaE2L7E1fjnDOSSVBG1fY-LT9i\" # Model\n",
        "!wget 'https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar'\n",
        "!tar -xvf 'scibert_scivocab_uncased.tar'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('tfworld.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('')\n",
        "\n",
        "!pip install transformers --quiet\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "print('TensorFlow:', tf.__version__)\n",
        "\n",
        "print('Loading Embeddings')\n",
        "citations_embeddings = np.load('CitationSimilarityVectors106Epochs.npy')\n",
        "abstract_embeddings = np.load('AbstractSimVectors.npy')\n",
        "assert citations_embeddings.shape == abstract_embeddings.shape\n",
        "\n",
        "normalizedC = tf.nn.l2_normalize(citations_embeddings, axis=1)\n",
        "normalizedA = tf.nn.l2_normalize(abstract_embeddings, axis=1) \n",
        "\n",
        "print('Loading Model')\n",
        "model = tf.saved_model.load('tfworld/inference_model/')\n",
        "print('laoding Tokenizer')\n",
        "tokenizer = BertTokenizer(vocab_file='scibert_scivocab_uncased/vocab.txt')\n",
        "\n",
        "print('Loading Semantic Scholar CS data. This will take a few more minutes, almost done . . .')\n",
        "df = pd.read_json('/content/TitlesIdsAbstractsEmbedIdsCOMPLETE_12-30-19.json.gzip', compression = 'gzip')\n",
        "embed2Title = pd.Series(df['title'].values,index=df['EmbeddingID']).to_dict()\n",
        "embed2Abstract = pd.Series(df['paperAbstract'].values,index=df['EmbeddingID']).to_dict()\n",
        "embed2Paper = pd.Series(df['id'].values,index=df['EmbeddingID']).to_dict()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10LV9QbZOkUyOzR4nh8hxesoKJhpmvpM9\n",
            "To: /content/CitationSimilarityVectors106Epochs.npy\n",
            "2.59GB [00:24, 106MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-23aNm7j0bnycvyd_OaQfofVYPTewgOI\n",
            "To: /content/AbstractSimVectors.npy\n",
            "2.59GB [00:45, 56.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NyUQwgUNj9bFsiCnZ2TfKmWn5r-Y6wav\n",
            "To: /content/TitlesIdsAbstractsEmbedIdsCOMPLETE_12-30-19.json.gzip\n",
            "432MB [00:05, 73.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wIRsAApaE2L7E1fjnDOSSVBG1fY-LT9i\n",
            "To: /content/tfworld.zip\n",
            "411MB [00:05, 74.4MB/s]\n",
            "--2020-01-09 01:54:17--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.241.176\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.241.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 442460160 (422M) [application/x-tar]\n",
            "Saving to: ‘scibert_scivocab_uncased.tar’\n",
            "\n",
            "scibert_scivocab_un 100%[===================>] 421.96M  36.0MB/s    in 12s     \n",
            "\n",
            "2020-01-09 01:54:29 (34.3 MB/s) - ‘scibert_scivocab_uncased.tar’ saved [442460160/442460160]\n",
            "\n",
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/vocab.txt\n",
            "scibert_scivocab_uncased/pytorch_model.bin\n",
            "scibert_scivocab_uncased/config.json\n",
            "\u001b[K     |████████████████████████████████| 450kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 48.9MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "TensorFlow 2.x selected.\n",
            "TensorFlow: 2.1.0-rc1\n",
            "Loading Embeddings\n",
            "Loading Model\n",
            "laoding Tokenizer\n",
            "Loading Semantic Scholar CS data. This will take a few more minutes, almost done . . .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVzix3jhPIYx",
        "colab_type": "text"
      },
      "source": [
        "Use this text box to search for papers. Our model was trained on using full abstracts using the 'query', so the model performs better with longer queries, but the model works surprisingly well with short queries as well. Give it a try."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lMVIjtwQm-1",
        "colab_type": "text"
      },
      "source": [
        "Our model was trained to use a citation emedding as a label, but we found out running similarity on our abstract embeddings results in surprisingly robust results as well, so we included both. The first half of the results are from the citation embeddings, the second half are from the abstract embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJJnXM5BiE-m",
        "colab_type": "code",
        "outputId": "fb8f8dc2-06b3-4fbf-8006-8f97ab65da44",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "query = \"Model for extracting an interpretable sentence embedding with self-attention. \" #@param {type:\"string\"}\n",
        "\n",
        "top_k_results = 50 #@param {type:\"integer\"}\n",
        "\n",
        "if top_k_results%2 == 0:\n",
        "    halfA = halfC = int(top_k_results/2)\n",
        "else:\n",
        "    halfC = int(top_k_results/2) + 1\n",
        "    halfA = int(top_k_results/2) \n",
        "\n",
        "abstract_encoded = tokenizer.encode(query, max_length=512, pad_to_max_length=True)\n",
        "abstract_encoded = tf.constant(abstract_encoded, dtype=tf.int32)[None, :]\n",
        "print('\\nQuery : ')\n",
        "pprint(query)\n",
        "\n",
        "s = time()\n",
        "bert_output = model(abstract_encoded)\n",
        "xq = tf.nn.l2_normalize(bert_output, axis=1)\n",
        "prediction_time = time() - s\n",
        "\n",
        "simNumpyC = np.matmul(normalizedC, tf.transpose(xq))\n",
        "simNumpyCTopK = (-simNumpyC[:,0]).argsort()[:halfC]\n",
        "simNumpyC_oTopK = -np.sort(-simNumpyC[:,0])[:halfC]\n",
        "allCit = np.vstack((simNumpyCTopK , simNumpyC_oTopK) )\n",
        "del simNumpyC\n",
        "\n",
        "simNumpyA = np.matmul(normalizedA, tf.transpose(xq))\n",
        "simNumpyATopK = (-simNumpyA[:,0]).argsort()[:halfA]\n",
        "simNumpyA_oTopK = -np.sort(-simNumpyA[:,0])[:halfA]\n",
        "allAbs = np.vstack((simNumpyATopK , simNumpyA_oTopK) )\n",
        "del simNumpyA\n",
        "\n",
        "allResults = np.concatenate((allAbs, allCit), axis = 1)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('------ Nearest papers  -----------------------------------------------------------')\n",
        "print('\\n')\n",
        "\n",
        "for embed in allResults[0]:\n",
        "    print('---------------')\n",
        "    print('-------')\n",
        "    print('---')\n",
        "    title = embed2Title[int(embed)]\n",
        "    abstractR = embed2Abstract[int(embed)]\n",
        "    paperId = embed2Paper[int(embed)]\n",
        "    print('Title: ', title)\n",
        "    print('\\nAbstract : ')\n",
        "    pprint(abstractR)\n",
        "    # print('\\n')\n",
        "    print('\\nLink: https://www.semanticscholar.org/paper/'+paperId)\n",
        "    print('---')\n",
        "    print('-------')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Query : \n",
            "'Model for extracting an interpretable sentence embedding with self-attention. '\n",
            "\n",
            "\n",
            "------ Nearest papers  -----------------------------------------------------------\n",
            "\n",
            "\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  iSentenizer: An incremental sentence boundary classifier\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we revisited the topic of sentence boundary detection, and '\n",
            " 'proposed an incremental approach to tackle the problem. The boundary '\n",
            " 'classifier is revised on the fly to adapt to the text of high variety of '\n",
            " 'sources and genres. We applied i+Learning, an incremental algorithm, for '\n",
            " 'constructing the sentence boundary detection model using different features '\n",
            " 'based on local context. Although the model can be easily trained on any '\n",
            " 'genre of text and on any alphabet language, we emphasize the ability that '\n",
            " 'the classifier is adaptable to text with domain and topic shifts without '\n",
            " 'retraining the whole model from scratch. Empirical results indicate that the '\n",
            " 'performance of proposed system is comparable to that of similar systems.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/3181562028ecab8692501aa04b5f6e1e0fead183\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Improved Sequential Dependency Analysis Integrating Labeling-Based Sentence Boundary Detection\n",
            "\n",
            "Abstract : \n",
            "('A dependency structure interprets modification relationships between words '\n",
            " 'or phrases and is recognized as an important element in semantic information '\n",
            " 'analysis. With the conventional approaches for extracting this dependency '\n",
            " 'structure, it is assumed that the complete sentence is known before the '\n",
            " 'analysis starts. For spontaneous speech data, however, this assumption is '\n",
            " 'not necessarily correct since sentence boundaries are not marked in the '\n",
            " 'data. Although sentence boundaries can be detected before dependency '\n",
            " 'analysis, this cascaded implementation is not suitable for online processing '\n",
            " 'since it delays the responses of the application. To solve these problems, '\n",
            " 'we proposed a sequential dependency analysis (SDA) method for online '\n",
            " 'spontaneous speech processing, which enabled us to analyze incomplete '\n",
            " 'sentences sequentially and detect sentence boundaries simultaneously. In '\n",
            " 'this paper, we propose an improved SDA integrating a labeling-based sentence '\n",
            " 'boundary detection (SntBD) technique based on Conditional Random Fields '\n",
            " '(CRFs). In the new method, we use CRF for soft decision of sentence '\n",
            " 'boundaries and combine it with SDA to retain its online framework. Since '\n",
            " 'CRF-based SntBD yields better estimates of sentence boundaries, SDA can '\n",
            " 'provide better results in which the dependency structure and sentence '\n",
            " 'boundaries are consistent. Experimental results using spontaneous lecture '\n",
            " 'speech from the Corpus of Spontaneous Japanese show that our improved SDA '\n",
            " 'outperforms the original SDA with SntBD accuracy providing better dependency '\n",
            " 'analysis results.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/816bc34c08e6bb8baa01e4f317d4aaadb3a604bd\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  The Effect of POS Tag Information on Sentence Boundary Detection in Turkish Texts\n",
            "\n",
            "Abstract : \n",
            "('Recently, Natural language processing (NLP) applications have been crucial '\n",
            " 'by the increase in the amount of digitized written and oral text documents. '\n",
            " 'As sentence boundary detection is the first step of most of the NLP '\n",
            " 'applications, it has high importance. In this study, the effects of using '\n",
            " 'POS (Part-of-Speech) tags on the performance of machine learning '\n",
            " 'methodsbased sentence boundary detection from Turkish texts have been '\n",
            " 'studied. To reach our goal, a dataset which contains 30000 instances such '\n",
            " 'that 15000 of them are sentences and the remaining 15000 instances are '\n",
            " 'non-sentence samples has been drawn from a subset of TNC (Turkish National '\n",
            " 'Corpus). The sub-corpus has 10.000.000 words in total, and to develop the '\n",
            " 'dataset, the characters which may represent the end of a sentence are '\n",
            " 'searched from the sub-corpus, then the text is divided into pieces from '\n",
            " 'these characters. Each piece is checked manually to label as sentence and '\n",
            " 'non-sentence, and randomly 30000 instances are selected to form the dataset. '\n",
            " 'Each instance in the dataset is converted to a vector by using total 9 '\n",
            " 'attributes that are used in the rule-based sentence boundary detection '\n",
            " 'studies and proposed in this study. After that two more attributes that are '\n",
            " 'POS tags of the terms before and after the haracter that may represent the '\n",
            " 'end of the sentence are included to the attribute set, and then the dataset '\n",
            " 'is again converted to vectors by using these 11 attributes. The twodatasets '\n",
            " 'are classified by using Back Propagation Neural Network, RBF Network, Naive '\n",
            " 'Bayes classifier, Decision Tree, and Support Vector Machines to evaluate the '\n",
            " 'performance of supervised learning methods on the sentence boundary '\n",
            " 'detection. After the experimental evaluation we observed that, when POS tags '\n",
            " 'are included, success of sentence boundary detection increases for all '\n",
            " 'classifiers, and the most successful classifier is decision tree with '\n",
            " 'classification accuracy which is improved from 84.7% to 86.2% when POS tags '\n",
            " 'are considered.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/14b31ff3917b32e6d15f1c04a734e32e82a5b138\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  A New Feature Extraction Approach Based on Sentence Element Analysis\n",
            "\n",
            "Abstract : \n",
            "('Considering that each sentence element of a sentence or clause plays an '\n",
            " 'important role in describing case and (or) object in documents, a feature '\n",
            " 'extraction method based on sentence element is proposed in this paper. The '\n",
            " 'method can extract feature terms from documents effectively and weight them '\n",
            " 'accurately. It first extracts sentence elements from dependency '\n",
            " 'relationships, and then selects and weights the terms according to the '\n",
            " 'sentence elements. Experimental results on a public dataset prove the '\n",
            " 'feasibility of our approach and demonstrate its advantage to feature '\n",
            " 'extraction method based on part of speech.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/d9cb323ad64f6b6cddfcb6e706df054a8f91e3ae\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Encoding Local Contexts of Sentences with Convolutions on pq-Gram Representations of Dependency Trees\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we present our sentence encoding approach that uses local '\n",
            " 'contexts constructed from pq-gram representations of a sentence’s dependency '\n",
            " 'tree. The context localization scope can be adjusted through parameters p, '\n",
            " 'the dependency depth, and q, the dependency width, which allows controlling '\n",
            " 'context-sensitivity. We show competitive results of using our sentence '\n",
            " 'encoding approach for sentence-pair modeling tasks.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/c090d6dcb7ec31edd7688a0a3628c68a686502e1\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  The analysis on mistaken segmentation of Tibetan words based on statistical method\n",
            "\n",
            "Abstract : \n",
            "('In this paper, by using the Tibetan word segmentation system, IEA-TWordSeg, '\n",
            " 'the authors attempt segmentation of the total 1271 sentences in the closed '\n",
            " 'set and 1000 sentences in an open set. The accuracy of testing is 99.54% and '\n",
            " '92.41% respectively. The authors describe the wrong segmentation types as '\n",
            " 'well as the causes of the mistakes, and demonstrate the proportion of '\n",
            " 'different types of segmentation errors. The purpose of the article is to '\n",
            " 'provide clues for those who intend to improve the accuracy of Tibetan word '\n",
            " 'segmentation system.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/a3f876619b321285c61244d5f8501c9ca4359cff\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Semantic role based sentence compression\n",
            "\n",
            "Abstract : \n",
            "('In this paper, a new unsupervised sentence compression method is proposed. '\n",
            " 'Sentences are tagged with Part Of Speech tags and semantic role labels. The '\n",
            " \"proposed method relies on the semantic roles of sentences' parts. Moreover, \"\n",
            " 'in the process of compression, other sentences in the context are taken into '\n",
            " 'account. The approach is applied in the context of multi-document '\n",
            " 'summarization. Experiments showed better results than other state of the art '\n",
            " 'approaches.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/c1b262231f1cd2dd38224a170123063980dc3750\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Extraction and categorization of transition information from large volume of texts using patterns and machine learning\n",
            "\n",
            "Abstract : \n",
            "('The information on transition of a thing is important for acquiring '\n",
            " 'knowledge on the thing. In this study, we extracted transition information '\n",
            " 'from a large number of sentences using pattern-based methods. We obtained an '\n",
            " 'F-measure of 0.86 for extraction of transition information by this method. '\n",
            " 'In order to improve the results, we then combined the pattern-based methods '\n",
            " 'with supervised machine-learning methods. We obtained F-measures of 0.91 and '\n",
            " '0.89 for extraction of transition information by support vector machine and '\n",
            " 'maximum entropy methods, respectively. Thus, we confirmed that the combined '\n",
            " 'approach outperformed the pattern-based method. We also categorized '\n",
            " 'transition information. In the experiments, we obtained an F-measure of 0.6 '\n",
            " 'for transition information categorization for categories containing many '\n",
            " 'events in the training data set. Furthermore, we examined sentences in terms '\n",
            " 'of conceptual changes in their transition information. We classified '\n",
            " 'transition information in various ways. The categories by the classification '\n",
            " 'provide a theoretical basis for future studies on transition information.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/66a3f6ad8c98ba24cf69957d22da65a7029d252d\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Legal Clause Extraction From Contract Using Machine Learning with Heuristics Improvement\n",
            "\n",
            "Abstract : \n",
            "('In this paper we are going to motivate and propose a novel task: Legal '\n",
            " 'Clause extraction from a contract document. For clause extraction, we '\n",
            " 'perform paragraph segmentation using paragraph boundary detection. In this '\n",
            " 'paper, we have discussed conventional paragraph detection approaches and '\n",
            " 'then describe the machine learning based approach. We detect paragraph '\n",
            " 'boundary using sequential classification algorithm. We also added varieties '\n",
            " 'of text feature which help to improve the accuracy of paragraph boundary '\n",
            " 'detection. To improve the accuracy of the model we have added false positive '\n",
            " 'samples and create a revised model. We introduce the legal clause extraction '\n",
            " 'system that performs the task with high accuracy.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/5142418193fb4fb21d46ac9d44479cb2816b30a9\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Representing sentence with unfolding recursive autoencoders and dynamic average pooling\n",
            "\n",
            "Abstract : \n",
            "('This paper proposes a new composition method to represent semantic '\n",
            " 'compositionality of sentences. Using the unfolding recursive autoencoders, '\n",
            " 'we build sentence representing trees from the original sentences of words. '\n",
            " 'We utilize trained word embeddings and sentence parser to train the model, '\n",
            " 'and we can build sentence representing trees from the trained model. We '\n",
            " 'further propose to use dynamic average pooling to pool the trees and get '\n",
            " 'fix-size vector representation for sentences. The fix-size vector '\n",
            " 'representation after dynamic average pooling can then be used to represent '\n",
            " 'sentences. We verify the validity of sentence representations by using them '\n",
            " 'to classify sentence paraphrase. Experiment shows that compared to the '\n",
            " 'baseline representation, using proposed representations together with '\n",
            " 'sub-vector Euclidean distance feature, the classification performance can be '\n",
            " 'improved by 1.39% for testing accuracy, which proves the proposed method can '\n",
            " 'better represent semantic compositionality of sentences.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/4e0a61b300408e721a3b0dad7dbd88a8f9587ece\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Case Based Construal using Minimal Features to Decipher Ambiguityin Punjabi Language\n",
            "\n",
            "Abstract : \n",
            "('A minimal feature set is taken in order to find the closest context of the '\n",
            " 'given ambiguous word by using case based construal model. The model proposed '\n",
            " 'in this paper uses case-based reasoning to sort out the similar cases with '\n",
            " 'vectors of size two (bigram), three (trigram) and four (ngram) using '\n",
            " 'Euclidean similarity function. These cases are then subjected to three '\n",
            " 'different classifiers, namely - Bayes, k-Nearest Neighbor, and Decision '\n",
            " 'Tree, to decipher the ambiguity.Vectorization eases the process of '\n",
            " 'disambiguation as compared to when full sentences are processed. The '\n",
            " 'similarity function helps to find similar cases of the given ambiguous word '\n",
            " 'whereas the three classifiers helps in finding the right context of the '\n",
            " 'given ambiguous word. Upon experimentation, Decision Tree classifier has '\n",
            " 'achieved an accuracy of 84.88% using pre-bigram vectors.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/d17c0c2e3b24f992fcbf5bc559e7a3cfd3459c46\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Semantic sentence embeddings for paraphrasing and text summarization\n",
            "\n",
            "Abstract : \n",
            "('This paper introduces a sentence to vector encoding framework suitable for '\n",
            " 'advanced natural language processing. Our latent representation is shown to '\n",
            " 'encode sentences with common semantic information with similar vector '\n",
            " 'representations. The vector representation is extracted from an '\n",
            " 'encoderdecoder model which is trained on sentence paraphrase pairs. We '\n",
            " 'demonstrate the application of the sentence representations for two '\n",
            " 'different tasks — sentence paraphrasing and paragraph summarization, making '\n",
            " 'it attractive for commonly used recurrent frameworks that process text. '\n",
            " 'Experimental results help gain insight how vector representations are '\n",
            " 'suitable for advanced language embedding.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/080ea4c468a982c73a7741abe59da5255c7d2c38\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Automatic abstracting important sentences of web articles\n",
            "\n",
            "Abstract : \n",
            "('Being increasingly popular, the Internet greatly changes our live. We can '\n",
            " 'conveniently receive and send information via the Internet. With the '\n",
            " 'information explosion in Web, it is becoming crucial to develop means to '\n",
            " 'automatically extract important sentences from the Web articles. In this '\n",
            " 'paper, we propose a method which uses both statistical and structural '\n",
            " 'information in sentence extraction. In addition, following the analysis of '\n",
            " \"human's extractions, several heuristic rules are added to filter out \"\n",
            " 'non-important sentences and to prevent similar sentences from being '\n",
            " 'extracted. Our experimental results proved the effectiveness of these means. '\n",
            " 'In particular, once the heuristic rules being added, a significant '\n",
            " 'improvement has been observed.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/a0e1a9b09cf050514ae1706d63cc77e2c44f683e\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Feature fluctuation absorption for a quick audio retrieval from long recordings\n",
            "\n",
            "Abstract : \n",
            "('Kashino et al. proposed (1999) a histogram-based quick signal search method '\n",
            " 'called time-series active search (TAS). TAS has only been effective in the '\n",
            " 'exact matching case, where the segments to be detected are assumed to be '\n",
            " 'exactly same as the reference signal. Here, we extend the method so that it '\n",
            " 'is applicable even if the features fluctuate. In addition to the feature '\n",
            " 'modification, feature dithering is discussed to absorb feature fluctuations. '\n",
            " 'Efficient time-scaled search is also investigated to cope with variations of '\n",
            " 'the reference signal duration. Tests using broadcast recordings show that '\n",
            " 'the extended method improves the accuracy in nonexact-matching tasks such as '\n",
            " \"hand-clap detection and word spotting in a single-speaker's narration. The \"\n",
            " 'tests also show the speed-ups by pruning introduced in the time-scaled '\n",
            " 'search.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/6b567ad9133da7fbd3ab1759e8a36570a4870772\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Context-Based Query Using Dependency Structures Based on Latent Topic Model\n",
            "\n",
            "Abstract : \n",
            "('To improve and enhance information retrieval on text database, there have '\n",
            " 'been many approaches proposed so far, but few investigation captures context '\n",
            " 'aspects of queries (of languages) directly. Here, we propose a new approach '\n",
            " 'to retrieve contextual dependencies in Japanese based on latent topic model. '\n",
            " 'The key idea comes from dependency structure which captures context in the '\n",
            " 'database and the queries. We examine some experimental results to see the '\n",
            " 'effectiveness.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/c2385b1c29ad5a75b0dc18a19f469505c99208b5\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Looking at projected documents: event detection & document identification\n",
            "\n",
            "Abstract : \n",
            "('In the context of a multimodal application, the article proposes an '\n",
            " 'image-based method for bridging the gap between document excerpts and video '\n",
            " 'extracts. The approach, called document image alignment, takes advantage of '\n",
            " 'the observable events related to documents that are visible during meetings. '\n",
            " 'In particular, the article presents a new method for detecting slide changes '\n",
            " 'in slideshows, its evaluation, and a preliminary work on document '\n",
            " 'identification.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/38f269c0f16c9ebfec4d2867153538804e841c83\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Research on Multi-document Summarization Based on LDA Topic Model\n",
            "\n",
            "Abstract : \n",
            "('Compared with VSM (Vector Space Model) and graph-ranking models, LDA (Latent '\n",
            " 'Dirichlet Allocation) Model can discover latent topics in the corpus and '\n",
            " 'latent topics are beneficial to use sentence-ranking mechanisms to form a '\n",
            " 'good summary. In the paper, based on LDA Model, a new method of '\n",
            " 'sentence-ranking is proposed. The method combines topic-distribution of each '\n",
            " 'sentence with topic-importance of the corpus together to calculate the '\n",
            " 'posterior probability of the sentence, and then, based on the posterior '\n",
            " 'probability, it selects sentences to form a summary. Topic-distribution of '\n",
            " 'each sentence represents the likelihood of sentence belonging to each topic '\n",
            " 'and topic-importance represents the degree that the topics cover the '\n",
            " 'significant portion of the corpus. The method highlights the latent topics '\n",
            " 'and optimizes the summarization. Experiment results on the dataset DUC2006 '\n",
            " 'show the advantage of the multi-document summarization algorithm proposed in '\n",
            " 'the paper. ROUGE values are improved compared with those methods, such as '\n",
            " 'LexRank, LDA-SIBS, LDA-PGS.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/4d8e43104005f75eedba687c7403b0cee71f8cdc\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Combining Statistics-Based and CNN-Based Information for Sentence Classification\n",
            "\n",
            "Abstract : \n",
            "('Sentence classification, serving as the foundation of the subsequent '\n",
            " 'text-based processing, continues attracting researchers attentions. '\n",
            " 'Recently, with the great success of deep learning, convolutional neural '\n",
            " 'network (CNN), a kind of common architecture of deep learning, has been '\n",
            " 'widely used to this filed and achieved excellent performance. However, most '\n",
            " 'CNN-based studies focus on using complex architectures to extract more '\n",
            " 'effective category information, requiring more time in training models. With '\n",
            " 'the aim to get better performance with less time cost on classification, '\n",
            " 'this paper proposes two simple and effective methods by fully combining '\n",
            " 'information both extracted from statistics and CNN. The first method is '\n",
            " 'S-SFCNN, which combines statistical features and CNN-based probabilistic '\n",
            " 'features of classification to build feature vectors, and then the vectors '\n",
            " 'are used to train the logistic regression classifiers. And the second method '\n",
            " 'is C-SFCNN, which combines CNN-based features and statistics-based '\n",
            " 'probabilistic features of classification to build feature vectors. In the '\n",
            " 'two methods, the Naive Bayes log-count ratios are selected as the text '\n",
            " 'statistical features and the single-layer and single channel CNN is used as '\n",
            " 'our CNN architecture. The testing results executed on 7 tasks show that our '\n",
            " 'methods can achieve better performance than many other complex CNN models '\n",
            " 'with less time cost. In addition, we summarized the main factors influencing '\n",
            " 'the performance of our methods though experiment.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/ee8992933f1bf398649909177523da8a6610882b\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Double-Hypergraph Based Sentence Ranking for Query-Focused Multi-document Summarizaton\n",
            "\n",
            "Abstract : \n",
            "('Traditional graph based sentence ranking approaches modeled the documents as '\n",
            " 'a text graph where vertices represent sentences and edges represent pairwise '\n",
            " 'similarity relationships between two sentences. Such modeling cannot capture '\n",
            " 'complex group relationships shared among multiple sentences which can be '\n",
            " 'useful for sentence ranking. In this paper, we propose two different group '\n",
            " 'relationships (sentence-topic relationship and document-topic relationship) '\n",
            " 'shared among sentences, and construct a double-hypergraph integrating these '\n",
            " 'relationships into a unified framework. Then, a double-hypergraph based '\n",
            " 'sentence ranking algorithm is developed for query-focused multi-document '\n",
            " 'summarization, in which Markov random walk is defined on each hypergraph and '\n",
            " 'the mixture Markov chains are formed so as to perform transductive learning '\n",
            " 'in the double-hypergraph. When evaluated on DUC datasets, performance of the '\n",
            " 'proposed approach is remarkable.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/e1427d72a7f1bd9658c01d0cf3c2358262ef0c5d\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Recording, Indexing, Summarizing, and Accessing Meeting Videos: An Overview of the AMI Project\n",
            "\n",
            "Abstract : \n",
            "('In this paper we give an overview of the AMI project. AMI developed the '\n",
            " 'following: (1) an infrastructure for recording meetings using multiple '\n",
            " 'microphones and cameras; (2) a one hundred hour, manually annotated meeting '\n",
            " 'corpus; (3) a number of techniques for indexing, and summarizing of meeting '\n",
            " 'videos using automatic speech recognition and computer vision, and (4) an '\n",
            " 'extensible framework for browsing, and searching of meeting videos. We give '\n",
            " 'an overview of the various techniques developed in AMI, their integration '\n",
            " 'into our meeting browser framework, and future plans for AMIDA (Augmented '\n",
            " 'Multiparty Interaction with Distant Access), the follow-up project to AMI.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/13d5859333de8bbeb16dac4d508805c5d320ef1c\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Personalized news through content augmentation and profiling\n",
            "\n",
            "Abstract : \n",
            "('This paper is concerned with the topic of personalized news assembly at the '\n",
            " 'set-top box, based on augmented video. This is video complemented with '\n",
            " 'additional information that is somehow relevant to the semantic video '\n",
            " 'content. We touch upon the technique that is used for video augmentation, '\n",
            " 'which is video subject detection followed by information searches on the '\n",
            " 'subject. The focus of this paper is on subject detection implemented using '\n",
            " 'traditional text analysis tools; video segmentation is based on these '\n",
            " 'results and visual processing. We describe the architecture of such a system '\n",
            " 'and the benefits to the consumer. Further we discuss a preliminary system '\n",
            " 'that shows the viability of the concept.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/c648fbe936539dcc5d1dcea96ed100d7846e1235\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Towards State-of-the-art Baselines for Vietnamese Multi-document Summarization\n",
            "\n",
            "Abstract : \n",
            "('Text summarization is challenging, but an interesting task of natural '\n",
            " 'language processing. While this task has been widely studied in English, it '\n",
            " 'is still an early stage in Vietnamese. This paper introduces an '\n",
            " 'investigation of extractive summarization methods in Vietnamese. To do that, '\n",
            " 'we implement and compare several well-known summarization methods in three '\n",
            " 'directions: unsupervised, supervised, and deep learning. We validate the '\n",
            " 'performance of the methods on two Vietnamese datasets. According to '\n",
            " 'experimental results, we find two interesting points. Firstly, '\n",
            " 'learning-to-rank methods achieve promising ROUGE-scores in many cases. '\n",
            " 'Particularly, one of them surpasses the state-of-the-art unsupervised '\n",
            " 'learning method. Secondly, formulating the scoring step in the form of '\n",
            " 'learning-to-rank benefits the selection step.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/85f1b883c4dc4af08d281147e290da5fc7bb1400\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Research on recognition of semantic chunk boundary in Tibetan\n",
            "\n",
            "Abstract : \n",
            "('Semantic chunk is able to well describe the sentence semantic framework. It '\n",
            " 'plays a very important role in Natural Language Processing applications, '\n",
            " 'such as machine translation, QA system and so on. At present, the Tibetan '\n",
            " 'chunk researches are mainly based on rule-methods. In this paper, according '\n",
            " 'to the distinctive language characteristics of Tibetan, we firstly put '\n",
            " 'forward the descriptive definition of the Tibetan semantic chunk and its '\n",
            " 'labeling scheme and then we propose a feature selection algorithm to select '\n",
            " 'the suitable ones automatically from the candidate feature-templates. '\n",
            " 'Through the experiment conducted on the two different kinds of Tibetan '\n",
            " 'corpus, namely corpus-sentence and corpus-discourse, the F-Measure achieves '\n",
            " '95.84%, 94.95% and 91.97%, 88.82% by using of Conditional Random Fields '\n",
            " '(CRF) model and Maximum Entropy (ME) model respectively. The positive '\n",
            " 'results show that the definition of Tibetan semantic chunk in this paper is '\n",
            " 'reasonable and operable. Furthermore, its boundary recognition is feasible '\n",
            " 'and effective via statistical techniques in small scale corpus.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/4890369815142a635736abc2960277e46bfe36dc\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Prosody-based sentence boundary detection in Chinese broadcast news\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we explore the use of prosodic features in sentence boundary '\n",
            " 'detection in Chinese broadcast news. The prosodic features include speaker '\n",
            " 'turn, music, pause duration, pitch, energy and speaking rate. Specifically, '\n",
            " 'considering the Chinese tonal effects in pitch trajectory, we propose to use '\n",
            " 'tone-normalized pitch features. Experiments using decision trees demonstrate '\n",
            " 'that the tone-normalized pitch features show superior performance in '\n",
            " 'sentence boundary detection in Chinese broadcast news. Furthermore, feature '\n",
            " 'combination is able to achieve apparent performance improvement by intuitive '\n",
            " 'feature interactive rules formed in the decision tree. Pause duration and a '\n",
            " 'tone-normalized pitch feature contribute the most part of the feature usage '\n",
            " 'in the best-performing decision tree.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/f503381a163e5477803e3d8d2a20b37c563eaf01\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Annotation of sentence structure\n",
            "\n",
            "Abstract : \n",
            "('The focus of this article is on the creation of a collection of sentences '\n",
            " 'manually annotated with respect to their sentence structure. We show that '\n",
            " 'the concept of linear segments—linguistically motivated units, which may be '\n",
            " 'easily detected automatically—serves as a good basis for the identification '\n",
            " 'of clauses in Czech. The segment annotation captures such relationships as '\n",
            " 'subordination, coordination, apposition and parenthesis; based on '\n",
            " 'segmentation charts, individual clauses forming a complex sentence are '\n",
            " 'identified. The annotation of a sentence structure enriches a '\n",
            " 'dependency-based framework with explicit syntactic information on relations '\n",
            " 'among complex units like clauses. We have gathered a collection of 3,444 '\n",
            " 'sentences from the Prague Dependency Treebank, which were annotated with '\n",
            " 'respect to their sentence structure (these sentences comprise 10,746 '\n",
            " 'segments forming 6,341 clauses). The main purpose of the project is to gain '\n",
            " 'a development data—promising results for Czech NLP tools (as a dependency '\n",
            " 'parser or a machine translation system for related languages) that adopt an '\n",
            " 'idea of clause segmentation have been already reported. The collection of '\n",
            " 'sentences with annotated sentence structure provides the possibility of '\n",
            " 'further improvement of such tools.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/32868659baf32a6be81fc6074c4e335c56be4185\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Hovering Controller Design for a Helicopter\n",
            "\n",
            "Abstract : \n",
            "('This paper studies the hovering controller design for a helicopter. The '\n",
            " 'linear helicopter motion equation is introduced. The hovering control law is '\n",
            " 'designed and the parameters mapping method is employed to search the best '\n",
            " 'available hover controller parameters in all flight states. The digital '\n",
            " 'simulation models are established. Simulation results are given to '\n",
            " 'demonstrate the effectiveness of the designed controllers. The hover '\n",
            " 'maneuver controlled by the designed controller passed the test flight.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/1b6538180e6621cec68ca9187ff2c68f33210824\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Pseudo-Identities and Bordered Words\n",
            "\n",
            "Abstract : \n",
            "('This paper investigates the notions of θ-bordered words and θ-unbordered '\n",
            " 'words for various pseudo-identity functions θ. A θ-bordered word is a '\n",
            " 'non-empty word u such that there exists a word v which is a prefix of u '\n",
            " 'while θ(v) is a suffix of u. The case where θ is the identity function '\n",
            " 'corresponds to the classical notions of bordered and unbordered words. Here '\n",
            " 'we explore cases where θ is a pseudo-identity function, such as a morphism '\n",
            " 'or antimorphism with the property θ = I, n ≥ 2, or a literal morphism or '\n",
            " 'antimorphism. We explore properties of θ-bordered and θ-unbordered words in '\n",
            " 'this context.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/8b638e8cb4bf31047de2629eceee91c457616a2e\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Effects of Adaptation Parameters on Convergence Time and Tolerance for Adaptive Threshold Elements\n",
            "\n",
            "Abstract : \n",
            "''\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/43150b8951f89c9a43e6ff8b04771625daf9c7f4\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Automatic segmentation of news items based on video and audio features\n",
            "\n",
            "Abstract : \n",
            "('The automatic segmentation of news items is a key for implementing the '\n",
            " 'automatic cataloging system of news video. This paper presents an approach '\n",
            " 'which manages audio and video feature information to automatically segment '\n",
            " 'news items. The integration of audio and visual analyses can overcome the '\n",
            " 'weakness of the approach using only image analysis techniques. It makes the '\n",
            " 'approach more adaptable to various situations of news items. The proposed '\n",
            " 'approach detects silence segments in accompanying audio, and integrates them '\n",
            " 'with shot segmentation results, as well as anchor shot detection results, to '\n",
            " 'determine the boundaries among news items. Experimental results show that '\n",
            " 'the integration of audio and video features is an effective approach to '\n",
            " 'solving the problem of automatic segmentation of news items.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/fb74daee751f2ef3a0a5b4734649c9d8e14bc332\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Word Semantic Similarity Calculation Based on Word2vec\n",
            "\n",
            "Abstract : \n",
            "('In order to solve the problem of poor universality and the absence of '\n",
            " 'contextual information in word similarity calculation based on dictionary, '\n",
            " 'this paper proposes a semantic similarity computation method based on '\n",
            " 'Word2vec. This method improves HowNet and Tongyici Cilin, and also adds the '\n",
            " 'word vector model as a weighing parameter to calculate the word similarity, '\n",
            " 'after compares the similarity of the words by assigning different weights to '\n",
            " 'the three methods. Through experimental comparison, the Pearson coefficient '\n",
            " 'of the algorithm and the artificial value is 0.892, and the method can cover '\n",
            " 'most words so that it can effectively solve the problem of the similarity of '\n",
            " 'the word calculation in the dictionary.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/0d7e3191b05d94b9ed24fa16c600527b1312286c\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  A texture-based method for document segmentation and classification\n",
            "\n",
            "Abstract : \n",
            "('In this paper we present a hybrid approach to segment and classify contents '\n",
            " 'of document images. A Document Image is segmented into three types of '\n",
            " 'regions: Graphics, Text and Space. The image of a document is subdivided '\n",
            " 'into blocks and for each block five GLCM (Grey Level Co-occurrence Matrix) '\n",
            " 'features are extracted. Based on these features, blocks are then clustered '\n",
            " 'into three groups using K-Means algorithm; connected blocks that belong to '\n",
            " 'the same group are merged. The classification of groups is done using '\n",
            " 'pre-learned heuristic rules. Experiments were conducted on scanned '\n",
            " 'newspapers and images from MediaTeam Document Database')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/33ae0c2ae9e7ad1d6fa3fcc82e1bd5a6c8204eba\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  An EMD-based long-term LSSVR for machine condition prognostics\n",
            "\n",
            "Abstract : \n",
            "('Machine condition prognostics is very important for system safety and '\n",
            " 'condition-based maintenance. Only one-step ahead forecasting of machine '\n",
            " 'condition is considered because of the poor performance of multi-step ahead '\n",
            " 'forecasting. To find effective method for non-stationary long-term machine '\n",
            " 'condition prognostics, this paper firstly reviews multi-step ahead '\n",
            " 'forecasting strategies followed with a comparison of different multi-step '\n",
            " 'ahead forecasting strategies using support vector regression (SVR) and least '\n",
            " 'squares support vector regression (LSSVR) in two datasets, and then develops '\n",
            " 'an recursive multi-step LSSVR (MSLSSVR) model by introducing empirical mode '\n",
            " 'decomposition (EMD) to realize machine condition prognostics. EMD is used to '\n",
            " 'get more stationary signals instead of the non-stationary original signal, '\n",
            " 'and MSLSSVR is constructed to make multi-step prediction with decomposed '\n",
            " 'signals individually. All predicted values are combined eventually to get '\n",
            " 'the future trajectory of condition indicator. Moreover, the proposed '\n",
            " 'algorithm is validated in the TE process. The experimental result shows that '\n",
            " 'the proposed EMD-MSLSSVR model can forecast the failure status in advance, '\n",
            " 'and the performance is satisfied.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/6d0d24d395488f84716dde54a8e322b9969a9afa\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Performance Analysis of Adaptive Processors via Diffusion Model Approximations and the Fokker-Planck Equation\n",
            "\n",
            "Abstract : \n",
            "('Some results on adaptive array processors and adaptive filters that have '\n",
            " 'been obtainsd by the use of diffusion model approximations and the '\n",
            " 'Fokker-Planck equation are described.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/2f0b49211312c80eefbd4be254bb0361441dca8a\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  On a special class of primitive words\n",
            "\n",
            "Abstract : \n",
            "('When representing DNA molecules as words, it is necessary to take into '\n",
            " 'account the fact that a word u encodes basically the same information as its '\n",
            " 'Watson-Crick complement @q(u), where @q denotes the Watson-Crick '\n",
            " 'complementarity function. Thus, an expression which involves only a word u '\n",
            " 'and its complement can be still considered as a repeating sequence. In this '\n",
            " 'context, we define and investigate the properties of a special class of '\n",
            " 'primitive words, called pseudo-primitive words relative to @q or simply '\n",
            " '@q-primitive words, which cannot be expressed as such repeating sequences. '\n",
            " 'For instance, we prove the existence of a unique @q-primitive root of a '\n",
            " 'given word, and we give some constraints forcing two distinct words to share '\n",
            " 'their @q-primitive root. Also, we present an extension of the well-known '\n",
            " 'Fine and Wilf theorem, for which we give an optimal bound.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/5de5e21bffc3ecfd1b8dcf36efd5169667b041f8\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Selective backbone construction for topology control in ad hoc networks\n",
            "\n",
            "Abstract : \n",
            "('A key step in controlling topology in ad hoc networks is the construction of '\n",
            " 'the backbone which is then used to transfer data. Nodes that are not part of '\n",
            " 'the backbone can then go to sleep to save energy and increase the lifetime '\n",
            " 'of the network. Centralized backbone construction algorithms give better '\n",
            " 'performance but incur high communication overhead, while localized '\n",
            " 'algorithms lack sufficient topology information needed to construct '\n",
            " 'efficient backbones. We present selective backbone construction (SBC) which '\n",
            " 'starts by selecting a small number of seed nodes in the backbone and then '\n",
            " 'completes its construction by making a sweep of the network spreading '\n",
            " 'outwards from the seed nodes. During the latter process, topology '\n",
            " 'information is transferred to allow better coordinator selection decisions. '\n",
            " 'We compared SBC with other power-saving protocols in a variety of tests '\n",
            " 'featuring different mobility levels, traffic patterns, and node densities. '\n",
            " 'Our experiments show that SBC is more efficient in saving energy and '\n",
            " 'extending network life while providing satisfactory network performance when '\n",
            " 'compared with 802.11, 802.11 PSM, and GAF.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/76058f93bc3b0c0c8491dfdd411f10592f81bc77\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  MAV navigation through indoor corridors using optical flow\n",
            "\n",
            "Abstract : \n",
            "('Safe navigation through corridors plays a major role in the autonomous use '\n",
            " 'of Micro Aerial Vehicles (MAVs) in indoor environments. In this paper, we '\n",
            " 'present an approach for wall collision avoidance using a depth map based on '\n",
            " 'optical flow from on board camera images. An omnidirectional fisheye camera '\n",
            " 'is used as a primary sensor, while IMU data is needed for compensating '\n",
            " 'rotational effects of the optical flow. The here presented approach is '\n",
            " 'designed for safely maneuvering a helicopter through an indoor corridor. '\n",
            " 'Results based on real images taken in a corridor with textured walls are '\n",
            " 'shown at the end of this paper.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/2acdf4466edc09f0e5de8e3a111da5f6d2a6fb45\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Pseudopalindrome closure operators in free monoids\n",
            "\n",
            "Abstract : \n",
            "('We consider involutory antimorphisms ϕ of a free monoid A* and their fixed '\n",
            " 'points, called ϕ-palindromes or pseudopalindromes. A ϕ-palindrome reduces to '\n",
            " 'a usual palindrome when ϕ is the reversal operator. For any word w ∈ A* the '\n",
            " 'right (resp. left) ϕ-palindrome closure of w is the shortest ϕ-palindrome '\n",
            " 'having w as a prefix (resp. suffix). We prove some results relating '\n",
            " 'ϕ-palindrome closure operators with periodicity and conjugacy, and derive '\n",
            " 'some interesting closure properties for the languages of finite Sturmian and '\n",
            " 'episturmian words. In particular, a finite word w is Sturmian if and only if '\n",
            " 'both its palindromic closures are so. Moreover, in such a case, both the '\n",
            " 'palindromic closures of w share the same minimal period of w. A new '\n",
            " 'characterization of finite Sturmian words follows, in terms of periodicity '\n",
            " 'and special factors of their palindromic closures. Some weaker results can '\n",
            " 'be extended to the episturmian case. By using the right ϕ-palindrome '\n",
            " 'closure, we extend the construction of standard episturmian words via '\n",
            " 'directive words. In this way one obtains a family of infinite words, called '\n",
            " 'ϕ-standard words, which are morphic images of episturmian words, as well as '\n",
            " 'a wider family of infinite words including the Thue-Morse word on two '\n",
            " 'symbols.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/4942a57224d3973b04f565ac0ea37d07791ea6c1\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Diameters of octahedra\n",
            "\n",
            "Abstract : \n",
            "('The Kolmogorov n-diameters of an m-dimensional octahedron Omα, i.e., of a '\n",
            " 'convex hull of vectors {±α1e1, ..., αmem}, are calculated.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/976c5bca6428475e1ce87fcb79b5c88c81352178\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  An improved adaboost face detection algorithm based on the different sample weights\n",
            "\n",
            "Abstract : \n",
            "('An improved face detection method is proposed on the basis of traditional '\n",
            " 'adaboost algorithm. The training samples are not distinguished in the '\n",
            " 'traditional face detection based on adaboost algorithm, which results in '\n",
            " 'ignoring face samples in the process of training and the face feature '\n",
            " \"information can't be fully shown. In addition, because face samples and \"\n",
            " 'non-face samples are treated equally, all samples must be calculated and the '\n",
            " 'time of training classifier is extended. In order to improve the bad '\n",
            " 'results, this paper proposes an improved strategy for implementation of '\n",
            " 'algorithm. Face samples and non-face samples are set different initial '\n",
            " 'weights when training classifier, so they attract different attention. And '\n",
            " 'face and non-face samples are handled separately in order to reduce the '\n",
            " 'complexity of the time. Compared with traditional methods, the improved '\n",
            " 'method spends less time on training classifier.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/17dbd30c2fb9800c4a5210c875f0b2c5e0c1d328\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Estimating value at risk with semiparametric support vector quantile regression\n",
            "\n",
            "Abstract : \n",
            "('Value at Risk (VaR) has been used as an important tool to measure the market '\n",
            " 'risk under normal market. Usually the VaR of log returns is calculated by '\n",
            " 'assuming a normal distribution. However, log returns are frequently found '\n",
            " 'not normally distributed. This paper proposes the estimation approach of VaR '\n",
            " 'using semiparametric support vector quantile regression (SSVQR) models which '\n",
            " 'are functions of the one-step-ahead volatility forecast and the length of '\n",
            " 'the holding period, and can be used regardless of the distribution. We find '\n",
            " 'that the proposed models perform better overall than the variance-covariance '\n",
            " 'and linear quantile regression approaches for return data on S&P 500, NIKEI '\n",
            " '225 and KOSPI 200 indices.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/3c1aba75b8828ee9ae10c83a5db46d569e6ebedc\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Fast Partial Distortion Elimination Algorithm for Lossless and Lossy Motion Estimation Using Hadamard Transform and Probability Model\n",
            "\n",
            "Abstract : \n",
            "('We intuitively derive DC and AC constraints to model local block '\n",
            " 'complexities using ordered Hadamard transform from pixel based gradient '\n",
            " 'method. For lossless motion estimation (ME), using (1), we obtain the '\n",
            " 'optimized search order in the matching error calculation by descending order '\n",
            " 'of local constraints using sum of these two constraints. LBC(k) = ACSum(k) + '\n",
            " '|DCMB - DCLB(k)|.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/e73a49ac7a1f47ae74ed5e67e2a851cd39909798\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  On (pa, pb, pa, pa-b)-Relative Difference Sets\n",
            "\n",
            "Abstract : \n",
            "('This paper provides new exponent and rank conditions for the existence of '\n",
            " 'abelian relative (pa,pb,pa,pa−b)-difference sets. It is also shown that no '\n",
            " 'splitting relative (22c,2d,22c,22c−d)-difference set exists if d > c and the '\n",
            " 'forbidden subgroup is abelian. Furthermore, abelian relative (16, 4, 16, '\n",
            " '4)-difference sets are studied in detail; in particular, it is shown that a '\n",
            " 'relative (16, 4, 16, 4)-difference set in an abelian group G ≇ Z8 × Z4 × Z2 '\n",
            " 'exists if and only if exp(G) ≤ 4 or G = Z8 × (Z2)3 with N ≅ Z2 × Z2.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/5c4916fe65b2494d9c4e2ea46762e1bea8678de8\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  On graphs of central episturmian words\n",
            "\n",
            "Abstract : \n",
            "('Episturmian sequences are a natural extension of Sturmian sequences to the '\n",
            " 'case of finite alphabets of arbitrary cardinality. In this paper, we are '\n",
            " 'interested in central episturmian words, or simply, epicentral words, i.e., '\n",
            " 'the palindromic prefixes of standard episturmian sequences. An epicentral '\n",
            " 'word admits a variety of faithful representations including as a directive '\n",
            " 'word, as a certain type of period vector, as a Parikh vector, as a certain '\n",
            " 'type of Fine and Wilf extremal word, as a suitable modular matrix, and as a '\n",
            " 'labeled graph. Various interconnections between the different '\n",
            " 'representations of an epicentral word are analyzed. In particular, we '\n",
            " 'investigate the structure of the graphs of epicentral words proving some '\n",
            " 'curious and surprising properties.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/ab0b1dba7204df0a40d86b671b9ffc8ff6ce2fa9\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  An Adaptive Filtering Method for Mixed Noise of Images\n",
            "\n",
            "Abstract : \n",
            "('Images are often affected by different noise interference in the '\n",
            " 'transmission process. The noise cause wrong information in image analysis. '\n",
            " 'To reduce noise, the simple and easy method is that receiving a few images '\n",
            " 'once time and averages those images to get an average image. But, this '\n",
            " 'method casts huge time cost. If it is difficult to obtain the image, the '\n",
            " 'method is more infeasible. The median filter has better performance for '\n",
            " 'impulse noise and the average filter has good results for lower Gaussian '\n",
            " 'noise. This paper presents a method based on median filter to filter extreme '\n",
            " 'value and designs an adaptive mask according to the number of extreme value '\n",
            " 'in the mask. Then, calculating the similarity for each pixel in the mask and '\n",
            " 'giving weights to pixels in the mask by the principle of proportionality. To '\n",
            " 'do so, the present method can achieve the goal which is removing '\n",
            " 'high-intensity impulse noise and reducing mixed noised interference.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/08125862ffab810b7b4f0c021e65b42b00b32d30\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  A tool to calculate the worst electromagnetic threat in cavities\n",
            "\n",
            "Abstract : \n",
            "('We have developed and implemented a fast code to calculate the worst case '\n",
            " 'electromagnetic threat inside shielded electromagnetically large rooms. The '\n",
            " 'code is numerical but based on experiments and analysis, and typical outputs '\n",
            " 'of the code are the maximum power and maximum electric fields inside the '\n",
            " 'shielded room. The code is validated by measurements.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/e742cd6ab09523d80bef38b36638daee26f9ac94\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Fine and Wilf words for any periods II\n",
            "\n",
            "Abstract : \n",
            "('Given positive integers n, and p\"1,...,p\"r, we establish a fast word '\n",
            " 'combinatorial algorithm for constructing a word w=w\"1...w\"n of length n, '\n",
            " 'with periods p\"1,...,p\"r, and on the maximal number of distinct letters. '\n",
            " 'Moreover, we show that the constructed word, which is unique up to word '\n",
            " 'isomorphism, is a pseudo-palindrome - i.e. it is a fixed point of an '\n",
            " 'involutory antimorphism.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/9095ee0ec27408249fa18c6db87fcd6818a7f039\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Calculation of Core Losses in Toroids with Rectangular Cross Section\n",
            "\n",
            "Abstract : \n",
            "('In this paper the electric and magnetic field distribution is investigated '\n",
            " 'in solid toroidal cores with rectangular cross section. An analytical method '\n",
            " 'is used in which the field distribution is calculated by means of orthogonal '\n",
            " 'expansion. Based on the results, the influence of various parameters, such '\n",
            " 'as frequency and conductivity of the core material, on the core losses is '\n",
            " 'discussed. The calculated results are compared with measured data, a good '\n",
            " 'agreement between the two values is achieved.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/8a3459b3f64f8308b299b3c23d394e3fbcedb50d\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Real time software based L1 C/A GPS receiver\n",
            "\n",
            "Abstract : \n",
            "('This paper gives a detailed description of a software based L1 C/A GPS '\n",
            " 'receiver, the described receiver based only on software, combine several '\n",
            " 'algorithms used to process incoming signals from GPS satellites, the '\n",
            " 'measurement chain has multiple blocks, signals acquisition to detect the '\n",
            " 'visible satellites, satellite tracking to handle the frequency shift due to '\n",
            " 'the Doppler Effect, corrected signals demodulation, information extraction '\n",
            " 'and reading to achieve precise visible satellites positions calculation, the '\n",
            " 'distinct satellite positions are used to find the receiver position based on '\n",
            " 'triangulation technique, the developed algorithms used to build our software '\n",
            " 'receiver are based on advanced signal processing methods, as Circular '\n",
            " 'correlation through Fast Fourier Transform, Delay lock loop DLL and Phase '\n",
            " 'lock loop PLL.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/d2371a378f3f4f0df91def0ad1dbf27b3284261b\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Guidance laws based on Lyapunov theory and DOB for three-dimensional missile-target interception\n",
            "\n",
            "Abstract : \n",
            "('In this paper, the three-dimensional terminal guidance problem of a missile '\n",
            " 'intercepting a maneuvering target is studied. Augmented proportional '\n",
            " 'navigation (PN) guidance laws based on Lyapunov stability theory are '\n",
            " 'presented. Augmented terms are target accelerations such that proposed '\n",
            " 'guidance laws have the ability to track the target maneuver. But, target '\n",
            " 'accelerations cannot be accurately obtained in practice. To solve this '\n",
            " 'problem, the nonlinear disturbance observer (NDOB) technology is introduced '\n",
            " 'to estimate target accelerations. Estimated target accelerations will be '\n",
            " 'employed as compensation terms in the proposed augmented PN guidance laws. '\n",
            " 'Composite guidance laws combining augmented PN guidance laws with NDOB are '\n",
            " 'proposed. Finally, simulation comparison results are provided to demonstrate '\n",
            " 'the effectiveness of the presented guidance laws.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/87760d3b60d7d86cc5b23a0366134463bede9711\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Text Classification via iVector Based Feature Representation\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we address the problem of text classification: classifying '\n",
            " 'modern machine-printed text, handwritten text and historical typewritten '\n",
            " 'text from degraded noisy documents. We propose a novel text classification '\n",
            " 'approach based on iVector, a newly developed concept in speaker '\n",
            " 'verification. To a given text line, the iVector is a fixed-length feature '\n",
            " 'vector representation, transformed from a high-dimensional super vector '\n",
            " 'based on means of Gaussian mixture model (GMM), where the text dependent '\n",
            " 'component is separated from a universal background model (UBM) and can be '\n",
            " 'represented by a low dimensional set of factors. We classify the text lines '\n",
            " 'with a discriminative classifier - support vector machine (SVM) in iVector '\n",
            " 'space. A baseline approach of text classification using GMM in feature space '\n",
            " 'is also presented for evaluation purpose. Experimental results on an Arabic '\n",
            " 'document database show accuracy of 92.04% for text line classification using '\n",
            " 'the proposed method. Furthermore, the relative word error rate (WER) of 9.6% '\n",
            " 'is decreased in optical character recognition (OCR) when coupled with the '\n",
            " 'proposed iVector-SVM classifier. The proposed iVector-SVM approach is '\n",
            " 'language independent, thus, can be applied to other scripts as well.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/c466c7f722bc989c1eb988898270381bcf4c6aa7\n",
            "---\n",
            "-------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}