{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoNaturalLanguageRecommendationsCPU-Autofeedback.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/NaturalLanguageRecommendations/blob/master/notebooks/inference/DemoNaturalLanguageRecommendationsCPU_Autofeedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9eERu8JP9r7",
        "colab_type": "text"
      },
      "source": [
        "This is our simple Colab demo notebook, which can run on a CPU instance, though it may crash a regular colab CPU instance since it's memory intensive. If that happens, a message should appear on the bottom left asking if you would like to switch to a 25 gb RAM instance, which will be more than enough memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3YuDoc3QUBv",
        "colab_type": "text"
      },
      "source": [
        "If you would like play with using a GPU or TPU for inference, please see our advanced demo notebook here https://colab.research.google.com/github/Santosh-Gupta/NaturalLanguageRecommendations/blob/master/notebooks/inference/build_index_and_search.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUfIc4xuOOpe",
        "colab_type": "text"
      },
      "source": [
        "### NOTE: This colab notebook automatically records queries submitted (anonymously), which will be used to optimize the next iterations of our model. If you wish not to automatically send queries, use this notebook instead https://colab.research.google.com/drive/1Es_peeshcFsReqFEVr12GIiGPKD0KnKY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8wAw2JV6F4Am",
        "outputId": "c556499b-95b8-4e21-fa57-6e5ef4519682",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        }
      },
      "source": [
        "#@title Download and load model, embeddings, and data, will take a several minutes. Double click on this to pop open the hood and checkout the code.\n",
        "\n",
        "!gdown --id \"10LV9QbZOkUyOzR4nh8hxesoKJhpmvpM9\"   # citation vectors\n",
        "# !gdown --id \"1-8gmT9cQpOUoZ_HzEaT9Xz6qfeVooAFn\"\n",
        "!gdown --id \"1-23aNm7j0bnycvyd_OaQfofVYPTewgOI\"   # abstract vectors\n",
        "!gdown --id \"1NyUQwgUNj9bFsiCnZ2TfKmWn5r-Y6wav\"   # TitlesIdAbstractsEmbedIds\n",
        "!gdown --id \"1wIRsAApaE2L7E1fjnDOSSVBG1fY-LT9i\" # Model\n",
        "!wget 'https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar'\n",
        "!tar -xvf 'scibert_scivocab_uncased.tar'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('tfworld.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('')\n",
        "\n",
        "!pip install transformers --quiet\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "print('TensorFlow:', tf.__version__)\n",
        "\n",
        "!gdown --id \"1owiHXcDyTYecOq0Y27bOk0s4jgxmukTs\"\n",
        "!pip install --upgrade --quiet gspread\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "scope = ['https://www.googleapis.com/auth/spreadsheets']\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name('worksheet1.worksheet2.worksheet3.json', scope)\n",
        "gc = gspread.authorize(credentials)\n",
        "worksheet1 = gc.open_by_key('1qlIZAAK3ZYTb20KeOr9f-TAxYz7yQnvHUsdGP9Iakrc').sheet1\n",
        "worksheet2 = gc.open_by_key('1AU37NTxsafd9GNhum2yR3iCux9nT9GAN5Bn4HaWcyU4').sheet1\n",
        "worksheet3 = gc.open_by_key('1Vaxn8rWz0CufCeDF_Ip9lzErZjK3AUA3g02fYMBe5P4').sheet1\n",
        "\n",
        "print('Loading Embeddings')\n",
        "citations_embeddings = np.load('CitationSimilarityVectors106Epochs.npy')\n",
        "abstract_embeddings = np.load('AbstractSimVectors.npy')\n",
        "assert citations_embeddings.shape == abstract_embeddings.shape\n",
        "\n",
        "normalizedC = tf.nn.l2_normalize(citations_embeddings, axis=1)\n",
        "normalizedA = tf.nn.l2_normalize(abstract_embeddings, axis=1) \n",
        "\n",
        "print('Loading Model')\n",
        "model = tf.saved_model.load('tfworld/inference_model/')\n",
        "print('laoding Tokenizer')\n",
        "tokenizer = BertTokenizer(vocab_file='scibert_scivocab_uncased/vocab.txt')\n",
        "\n",
        "print('Loading Semantic Scholar CS data, almost done . . .')\n",
        "df = pd.read_json('/content/TitlesIdsAbstractsEmbedIdsCOMPLETE_12-30-19.json.gzip', compression = 'gzip')\n",
        "embed2Title = pd.Series(df['title'].values,index=df['EmbeddingID']).to_dict()\n",
        "embed2Abstract = pd.Series(df['paperAbstract'].values,index=df['EmbeddingID']).to_dict()\n",
        "embed2Paper = pd.Series(df['id'].values,index=df['EmbeddingID']).to_dict()\n",
        "\n",
        "import sys, os\n",
        "\n",
        "# Disable\n",
        "def blockPrint():\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "# Restore\n",
        "def enablePrint():\n",
        "    sys.stdout = sys.__stdout__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10LV9QbZOkUyOzR4nh8hxesoKJhpmvpM9\n",
            "To: /content/CitationSimilarityVectors106Epochs.npy\n",
            "2.59GB [00:38, 68.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-23aNm7j0bnycvyd_OaQfofVYPTewgOI\n",
            "To: /content/AbstractSimVectors.npy\n",
            "2.59GB [00:42, 60.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NyUQwgUNj9bFsiCnZ2TfKmWn5r-Y6wav\n",
            "To: /content/TitlesIdsAbstractsEmbedIdsCOMPLETE_12-30-19.json.gzip\n",
            "432MB [00:05, 82.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wIRsAApaE2L7E1fjnDOSSVBG1fY-LT9i\n",
            "To: /content/tfworld.zip\n",
            "411MB [00:04, 85.2MB/s]\n",
            "--2020-01-12 20:27:42--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.237.80\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.237.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 442460160 (422M) [application/x-tar]\n",
            "Saving to: ‘scibert_scivocab_uncased.tar’\n",
            "\n",
            "scibert_scivocab_un 100%[===================>] 421.96M  23.0MB/s    in 19s     \n",
            "\n",
            "2020-01-12 20:28:02 (22.0 MB/s) - ‘scibert_scivocab_uncased.tar’ saved [442460160/442460160]\n",
            "\n",
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/vocab.txt\n",
            "scibert_scivocab_uncased/pytorch_model.bin\n",
            "scibert_scivocab_uncased/config.json\n",
            "\u001b[K     |████████████████████████████████| 450kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 12.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 21.2MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "TensorFlow 2.x selected.\n",
            "TensorFlow: 2.1.0-rc1\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1owiHXcDyTYecOq0Y27bOk0s4jgxmukTs\n",
            "To: /content/worksheet1.worksheet2.worksheet3.json\n",
            "100% 2.35k/2.35k [00:00<00:00, 1.74MB/s]\n",
            "Loading Embeddings\n",
            "Loading Model\n",
            "laoding Tokenizer\n",
            "Loading Semantic Scholar CS data, almost done . . .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVzix3jhPIYx",
        "colab_type": "text"
      },
      "source": [
        "Use cell below to search for papers. Our model was trained on using full abstracts using the 'query', so the model performs better with longer queries, but the model works surprisingly well with short queries as well. Give it a try."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lMVIjtwQm-1",
        "colab_type": "text"
      },
      "source": [
        "Our model was trained to use a citation emedding as a label, but we found out running similarity on our abstract embeddings results in surprisingly robust results as well, so we included both. The first half of the results are from the citation embeddings, the second half are from the abstract embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJJnXM5BiE-m",
        "colab_type": "code",
        "outputId": "82fb74e7-e917-4ce7-9e7a-d55739a93505",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "query = \"Vector representation of text for information retrieval. Document embeddings for search. Vector representation of query. Embedding representation of queries. \" #@param {type:\"string\"}\n",
        "\n",
        "top_k_results = 50 #@param {type:\"integer\"}\n",
        "\n",
        "if top_k_results%2 == 0:\n",
        "    halfA = halfC = int(top_k_results/2)\n",
        "else:\n",
        "    halfC = int(top_k_results/2) + 1\n",
        "    halfA = int(top_k_results/2) \n",
        "\n",
        "abstract_encoded = tokenizer.encode(query, max_length=512, pad_to_max_length=True)\n",
        "abstract_encoded = tf.constant(abstract_encoded, dtype=tf.int32)[None, :]\n",
        "print('\\nQuery : ')\n",
        "pprint(query)\n",
        "\n",
        "s = time()\n",
        "bert_output = model(abstract_encoded)\n",
        "xq = tf.nn.l2_normalize(bert_output, axis=1)\n",
        "prediction_time = time() - s\n",
        "\n",
        "simNumpyC = np.matmul(normalizedC, tf.transpose(xq))\n",
        "simNumpyCTopK = (-simNumpyC[:,0]).argsort()[:halfC]\n",
        "simNumpyC_oTopK = -np.sort(-simNumpyC[:,0])[:halfC]\n",
        "allCit = np.vstack((simNumpyCTopK , simNumpyC_oTopK) )\n",
        "del simNumpyC\n",
        "\n",
        "simNumpyA = np.matmul(normalizedA, tf.transpose(xq))\n",
        "simNumpyATopK = (-simNumpyA[:,0]).argsort()[:halfA]\n",
        "simNumpyA_oTopK = -np.sort(-simNumpyA[:,0])[:halfA]\n",
        "allAbs = np.vstack((simNumpyATopK , simNumpyA_oTopK) )\n",
        "del simNumpyA\n",
        "\n",
        "allResults = np.concatenate((allAbs, allCit), axis = 1)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('------ Nearest papers  -----------------------------------------------------------')\n",
        "print('\\n')\n",
        "\n",
        "for embed in allResults[0]:\n",
        "    print('---------------')\n",
        "    print('-------')\n",
        "    print('---')\n",
        "    title = embed2Title[int(embed)]\n",
        "    abstractR = embed2Abstract[int(embed)]\n",
        "    paperId = embed2Paper[int(embed)]\n",
        "    print('Title: ', title)\n",
        "    print('\\nAbstract : ')\n",
        "    pprint(abstractR)\n",
        "    # print('\\n')\n",
        "    print('\\nLink: https://www.semanticscholar.org/paper/'+paperId)\n",
        "    print('---')\n",
        "    print('-------')\n",
        "\n",
        "blockPrint()\n",
        "values_list = worksheet1.col_values(3)\n",
        "worksheet1.update_cell(len(values_list)+1, 3, query)\n",
        "enablePrint()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Query : \n",
            "('Vector representation of text for information retrieval. Document embeddings '\n",
            " 'for search. Vector representation of query. Embedding representation of '\n",
            " 'queries. ')\n",
            "\n",
            "\n",
            "------ Nearest papers  -----------------------------------------------------------\n",
            "\n",
            "\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Discriminative features for document classification\n",
            "\n",
            "Abstract : \n",
            "('Document representation using the bag-of-words approach may require bringing '\n",
            " 'the dimensionality of the representation down in order to be able to make '\n",
            " 'effective use of various statistical classification methods. Latent Semantic '\n",
            " 'Indexing (LSI) is one such method that is based on eigendecomposition of the '\n",
            " 'covariance of the document-term matrix. Another often used approach is to '\n",
            " 'select a small number of most important features out of the whole set '\n",
            " 'according to some relevant criterion. This paper points out that LSI ignores '\n",
            " 'discrimination while concentrating on representation. Furthermore, selection '\n",
            " 'methods fail to produce a feature set that jointly optimizes class '\n",
            " 'discrimination. As a remedy, we suggest supervised linear discriminative '\n",
            " 'transforms, and report good classification results applying these to the '\n",
            " 'Reuters-21578 database.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/df9f4ec4924efc74b57de627f4126fc571e56a99\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Text Classification Based On Word Subspace With Term-Frequency\n",
            "\n",
            "Abstract : \n",
            "('Text classification has become indispensable due to the rapid increase of '\n",
            " 'text in digital form. Over the past three decades, efforts have been made to '\n",
            " 'approach this task using various learning algorithms and statistical models '\n",
            " 'based on bag-of-words (BOW) features. Despite its simple implementation, BOW '\n",
            " 'features lack of semantic meaning representation. To solve this problem, '\n",
            " 'neural networks started to be employed to learn word vectors, such as the '\n",
            " 'word2vec. Word2vec embeds word semantic structure into vectors, where the '\n",
            " 'angle between vectors indicates the meaningful similarity between words. To '\n",
            " 'measure the similarity between texts, we propose the novel concept of word '\n",
            " 'subspace, which can represent the intrinsic variability of features in a set '\n",
            " 'of word vectors. Through this concept, it is possible to model text from '\n",
            " 'word vectors while holding semantic information. To incorporate the word '\n",
            " 'frequency directly in the subspace model, we further extend the word '\n",
            " 'subspace to the term-frequency (TF) weighted word subspace. Based on these '\n",
            " 'new concepts, text classification can be performed under the mutual subspace '\n",
            " 'method (MSM) framework. The validity of our modeling is shown through '\n",
            " 'experiments on the Reuters text database, comparing the results to various '\n",
            " 'state-of-art algorithms.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/a627c0ef215880857e65f116b9670c3242e90e09\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Simultaneous Similarity Learning and Feature-Weight Learning for Document Clustering\n",
            "\n",
            "Abstract : \n",
            "('A key problem in document classification and clustering is learning the '\n",
            " 'similarity between documents. Traditional approaches include estimating '\n",
            " 'similarity between feature vectors of documents where the vectors are '\n",
            " 'computed using TF-IDF in the bag-of-words model. However, these approaches '\n",
            " 'do not work well when either similar documents do not use the same '\n",
            " 'vocabulary or the feature vectors are not estimated correctly. \\n'\n",
            " ' \\n'\n",
            " 'In this paper, we represent documents and keywords using multiple layers of '\n",
            " 'connected graphs. We pose the problem of simultaneously learning similarity '\n",
            " 'between documents and keyword weights as an edge-weight regularization '\n",
            " 'problem over the different layers of graphs. Unlike most feature weight '\n",
            " 'learning algorithms, we propose an unsupervised algorithm in the proposed '\n",
            " 'framework to simultaneously optimize similarity and the keyword weights. We '\n",
            " 'extrinsically evaluate the performance of the proposed similarity measure on '\n",
            " 'two different tasks, clustering and classification. The proposed similarity '\n",
            " 'measure outperforms the similarity measure proposed by (Muthukrishnan et '\n",
            " 'al., 2010), a state-of-the-art classification algorithm (Zhou and Burges, '\n",
            " '2007) and three different baselines on a variety of standard, large data '\n",
            " 'sets.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/f3f03effa37cb5b1efd496928bf3ab497063cd80\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Forecasting and discriminant analysis\n",
            "\n",
            "Abstract : \n",
            "('Document representation using the bag-of-words approach may require bringing '\n",
            " 'the dimensionality of the representation down in order to be able to make '\n",
            " 'effective use of various statistical classification methods. Latent Semantic '\n",
            " 'Indexing (LSI) is one such method that is based on eigendecomposition of the '\n",
            " 'covariance of the document-term matrix. This paper points out that LSI '\n",
            " 'ignores discrimination while concentrating on representation.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/03af0e66fcde37169b78a2e770c0885d7c7aebed\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Fuzzy Bag-of-Words Model for Document Representation\n",
            "\n",
            "Abstract : \n",
            "('One key issue in text mining and natural language processing is how to '\n",
            " 'effectively represent documents using numerical vectors. One classical model '\n",
            " 'is the Bag-of-Words (BoW). In a BoW-based vector representation of a '\n",
            " 'document, each element denotes the normalized number of occurrence of a '\n",
            " 'basis term in the document. To count the number of occurrence of a basis '\n",
            " 'term, BoW conducts exact word matching, which can be regarded as a hard '\n",
            " 'mapping from words to the basis term. BoW representation suffers from its '\n",
            " 'intrinsic extreme sparsity, high dimensionality, and inability to capture '\n",
            " 'high-level semantic meanings behind text data. To address the aforementioned '\n",
            " 'issues, we propose a new document representation method named fuzzy '\n",
            " 'Bag-of-Words (FBoW) in this paper. FBoW adopts a fuzzy mapping based on '\n",
            " 'semantic correlation among words quantified by cosine similarity measures '\n",
            " 'between word embeddings. Since word semantic matching instead of exact word '\n",
            " 'string matching is used, the FBoW could encode more semantics into the '\n",
            " 'numerical representation. In addition, we propose to use word clusters '\n",
            " 'instead of individual words as basis terms and develop fuzzy '\n",
            " 'Bag-of-WordClusters (FBoWC) models. Three variants under the framework of '\n",
            " 'FBoWC are proposed based on three different similarity measures between word '\n",
            " 'clusters and words, which are named as <inline-formula><tex-math '\n",
            " 'notation=\"LaTeX\">$\\\\text{FBoWC}_{\\\\rm mean}$</tex-math></inline-formula>, '\n",
            " '<inline-formula><tex-math notation=\"LaTeX\">$\\\\text{FBoWC}_{\\\\rm '\n",
            " 'max}$</tex-math></inline-formula>, and <inline-formula><tex-math '\n",
            " 'notation=\"LaTeX\">$\\\\text{FBoWC}_{\\\\rm min}$</tex-math></inline-formula>, '\n",
            " 'respectively. Document representations learned by the proposed FBoW and '\n",
            " 'FBoWC are dense and able to encode high-level semantics. The task of '\n",
            " 'document categorization is used to evaluate the performance of learned '\n",
            " 'representation by the proposed FBoW and FBoWC methods. The results on seven '\n",
            " 'real-word document classification datasets in comparison with six document '\n",
            " 'representation learning methods have shown that our methods FBoW and FBoWC '\n",
            " 'achieve the highest classification accuracies.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/acfc092dc98ce90ff50655716cd5e257a3b9ca45\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Local Word Bag Model for Text Categorization\n",
            "\n",
            "Abstract : \n",
            "('Many text processing applications adopted the bag of words (BOW) model '\n",
            " 'representation of documents, in which each document is represented as a '\n",
            " 'vector of weighted terms or n-grams, and then the cosine distance between '\n",
            " 'two vectors is used as the similarity measurement. Although the great '\n",
            " 'success in information retrieval and text categorization, the conventional '\n",
            " 'BOW model ignores the detailed local text information, i.e. the '\n",
            " 'co-occurrence pattern of words at sentence or paragraph level. In this '\n",
            " 'paper, we propose a novel approach to represent a document as a set of local '\n",
            " 'tf-idf vectors, or what we called local word bags (LWB). By encapsulating '\n",
            " 'local information distributed around a document into multiple LWBs, we can '\n",
            " 'measure the similarity of two documents via the partial match of their '\n",
            " 'corresponding local bags. To perform the matching efficiently, we introduce '\n",
            " 'the local word bag kernel (LWB kernel), a variant of VG-Pyramid match '\n",
            " 'kernel. The new kernel enables the discriminative machine learning methods '\n",
            " 'like SVM to compute the partial matching between two sets of LWBs in linear '\n",
            " 'time after an one time hierarchical clustering procedure over all local bags '\n",
            " 'at the initialization stage. Experiments on real world datasets demonstrate '\n",
            " 'the effectiveness of our new approach.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/ca8c2b9ea7f98b95260ee36583fa4fbed5942ade\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Discrimination of Old Document Images Using Their Style\n",
            "\n",
            "Abstract : \n",
            "('Based on the principle described by Pareti et al. in [1], [2], and by '\n",
            " 'Chouaib et al. in [3], this paper proposes to combine the use of the Zipf '\n",
            " 'law and the use of bag of patterns for the implementation of a document '\n",
            " 'indexing processing scheme. Contrarily to these two mentioned approaches, we '\n",
            " 'retain the most important patterns based on the TF-IDF criteria, and the '\n",
            " 'pattern selection is local. This paper presents the different stages of our '\n",
            " 'indexing process, as well as their application to historical documents. '\n",
            " 'Results on comlex images are given, illustrated and discussed.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/41a495d7a5f45ecf15bb7d32a3a10beaca96667f\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Discriminative features for text\n",
            "document classification\n",
            "\n",
            "Abstract : \n",
            "('AbstractThe bag-of-words approach to text document representation\\n'\n",
            " 'typically results in vectors of the order of 5000–20,000\\n'\n",
            " 'components as the representation of documents. To make effective\\n'\n",
            " 'use of various statistical classifiers, it may be necessary to\\n'\n",
            " 'reduce the dimensionality of this representation. We point out\\n'\n",
            " 'deficiencies in class discrimination of two popular such\\n'\n",
            " 'methods, Latent Semantic Indexing (LSI), and sequential feature\\n'\n",
            " 'selection according to some relevant criterion. As a remedy, we\\n'\n",
            " 'suggest feature transforms based on Linear Discriminant Analysis\\n'\n",
            " '(LDA). Since LDA requires operating both with large and dense\\n'\n",
            " 'matrices, we propose an efficient intermediate dimension\\n'\n",
            " 'reduction step using either a random transform or LSI. We report\\n'\n",
            " 'good classification results with the combined feature transform\\n'\n",
            " 'on a subset of the Reuters-21578 database. Drastic reduction of\\n'\n",
            " 'the feature vector dimensionality from 5000 to 12 actually\\n'\n",
            " 'improves the classification performance.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/d20e8a2deeb7546a86142721cc5d79528bb8ecb7\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Bag of Embedded Words learning for text retrieval\n",
            "\n",
            "Abstract : \n",
            "('The word embedding models are capable of capturing the semantic content of '\n",
            " 'the textual words. The process of extracting a set of word embedding vectors '\n",
            " 'from a text document is similar to the feature extraction step of the '\n",
            " 'Bag-of-Features pipeline, which is usually used in computer vision tasks. '\n",
            " 'That gives rise to the Bag-of-Embedded Words (BoEW) model. In this paper a '\n",
            " 'novel learning technique that optimizes both the word embedding and the '\n",
            " 'codebook of the BoEW model towards text retrieval is proposed. The proposed '\n",
            " 'method adheres to the cluster hypothesis that states that points in the same '\n",
            " 'cluster are likely to fulfill the same information need and it is '\n",
            " 'demonstrated, using two text datasets, that can significantly increase the '\n",
            " 'retrieval precision. Finally, the proposed technique uses smaller '\n",
            " 'representations than the competitive representation methods, that allows to '\n",
            " 'reduce both the retrieval time and the storage requirements.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/88120fb6baac0c9de4bad7dbb0ac6bc2a4edba1e\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Embedding Document Structure to Bag-of-Words through Pair-wise Stable Key-Regions\n",
            "\n",
            "Abstract : \n",
            "('Since the document structure carries valuable discriminative information, '\n",
            " 'plenty of efforts have been made for extracting and understanding document '\n",
            " 'structure among which layout analysis approaches are the most commonly used. '\n",
            " 'In this paper, Distance Transform based MSER (DTMSER) is employed to '\n",
            " 'efficiently extract the document structure as a dendrogram of key-regions '\n",
            " 'which roughly correspond to structural elements such as characters, words '\n",
            " 'and paragraphs. Inspired by the Bag of Words (BoW) framework, we propose an '\n",
            " 'efficient method for structural document matching by representing the '\n",
            " 'document image as a histogram of key-region pairs encoding structural '\n",
            " 'relationships. Applied to the scenario of document image retrieval, '\n",
            " 'experimental results demonstrate a remarkable improvement when comparing the '\n",
            " 'proposed method with typical BoW and pyramidal BoW methods.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/e67b3e09b8966ca4257e0e7eaf70dd10d692f2e2\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Efficient Semantic Indexing for Image Retrieval\n",
            "\n",
            "Abstract : \n",
            "('Semantic analysis of a document collection can be viewed as an unsupervised '\n",
            " 'clustering of the constituent words and documents around hidden or latent '\n",
            " 'concepts. This has shown to improve the performance of visual bag of words '\n",
            " 'in image retrieval. However, the enhancement in performance depends heavily '\n",
            " 'on the right choice of number of semantic concepts. Most of the semantic '\n",
            " 'indexing schemes are also computationally costly. In this paper, we employ a '\n",
            " 'bipartite graph model (BGM) for image retrieval. BGM is a scalable data '\n",
            " 'structure that aids semantic indexing in an efficient manner. It can also be '\n",
            " 'incrementally updated. BGM uses \\\\textbf{tf-idf} values for building a '\n",
            " 'semantic bipartite graph. We also introduce a graph partitioning algorithm '\n",
            " 'that works on the BGM to retrieve semantically relevant images from a '\n",
            " 'database. We demonstrate the properties as well as performance of our '\n",
            " 'semantic indexing scheme through a series of experiments. We also compare '\n",
            " 'our methods with incremental pLSA.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/cc1457bcbc623cd48a28fe167bfdbaf13ad05f46\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Taming Wild High Dimensional Text Data with a Fuzzy Lash\n",
            "\n",
            "Abstract : \n",
            "('The bag of words (BOW) represents a corpus in a matrix whose elements are '\n",
            " 'the frequency of words. However, each row in the matrix is a very '\n",
            " 'high-dimensional sparse vector. Dimension reduction (DR) is a popular method '\n",
            " 'to address sparsity and high-dimensionality issues. Among different '\n",
            " 'strategies to develop DR method, Unsupervised Feature Transformation (UFT) '\n",
            " 'is a popular strategy to map all words on a new basis to represent BOW. The '\n",
            " 'recent increase of text data and its challenges imply that DR area still '\n",
            " 'needs new perspectives. Although a wide range of methods based on the UFT '\n",
            " 'strategy has been developed, the fuzzy approach has not been considered for '\n",
            " 'DR based on this strategy. This research investigates the application of '\n",
            " 'fuzzy clustering as a DR method based on the UFT strategy to collapse BOW '\n",
            " 'matrix to provide a lower-dimensional representation of documents instead of '\n",
            " 'the words in a corpus. The quantitative evaluation shows that fuzzy '\n",
            " 'clustering produces superior performance and features to Principal '\n",
            " 'Components Analysis (PCA) and Singular Value Decomposition (SVD), two '\n",
            " 'popular DR methods based on the UFT strategy.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/e69d6dd897fbcc466c611e122acbbf23c7404675\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  A Graph Lattice Approach to Maintaining Dense Collections of Subgraphs as Image Features\n",
            "\n",
            "Abstract : \n",
            "('Document classification and indexing methods depend on having informative '\n",
            " 'image features. This paper shows how large families of complex features can '\n",
            " 'be built out of simpler ones through construction of a graph lattice -- a '\n",
            " 'hierarchy of related sub graphs linked in a lattice. A graph lattice enables '\n",
            " 'efficiency gains that make it possible to effectively employ bag-of-words '\n",
            " 'methods for document classification using high-dimensional feature vectors. '\n",
            " 'Each feature is itself a subgraph, and a feature vector is a count of '\n",
            " 'occurrences of sub graphs in the image. The graph lattice enables methods '\n",
            " 'for adaptively growing a feature space of sub graphs tailored to observed '\n",
            " 'document genres. We demonstrate the approach through classification of forms '\n",
            " 'containing rectilinear line art.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/9fc72b732e8f35c8290e018d348086ffbd7c2f0f\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  A Bag-of-Pages Approach to Unordered Multi-page Document Classification\n",
            "\n",
            "Abstract : \n",
            "('We consider the problem of classifying documents containing multiple '\n",
            " 'unordered pages. For this purpose, we propose a novel bag-of-pages document '\n",
            " 'representation. To represent a document, one assigns every page to a '\n",
            " 'prototype in a codebook of pages. This leads to a histogram representation '\n",
            " 'which can then be fed to any discriminative classifier. We also consider '\n",
            " 'several refinements over this initial approach. We show on two challenging '\n",
            " 'datasets that the proposed approach significantly outperforms a baseline '\n",
            " 'system.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/a15319d66c366fba17adc5a152e308b8b1083eec\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Document Specific Sparse Coding for Word Retrieval\n",
            "\n",
            "Abstract : \n",
            "('Bag of words (BoW) based retrieval is an efficient method to compare the '\n",
            " 'visual similarity between two images. Recognition free methods based on BoW '\n",
            " 'have shown to outperform OCR based methods. We further improve the '\n",
            " 'performance by defining a document specific sparse coding scheme for '\n",
            " 'representing visual words (interest points) in document images. Our method '\n",
            " 'is motivated by the successful use of sparsity in signal representation by '\n",
            " 'exploiting the neighbourhood properties. In addition to providing insights '\n",
            " 'into the design of the coding scheme, we also verify the method on two data '\n",
            " 'sets and compare with the recent methods. We have also developed text query '\n",
            " 'based search solution, and we report performance comparable to image based '\n",
            " 'search.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/b71cfd2d8cbf9d45e436d03f7b3583ead3b9e58c\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Semantic hashing\n",
            "\n",
            "Abstract : \n",
            "('We show how to learn a deep graphical model of the word-count vectors '\n",
            " 'obtained from a large set of documents. The values of the latent variables '\n",
            " 'in the deepest layer are easy to infer and give a much better representation '\n",
            " 'of each document than Latent Semantic Analysis. When the deepest layer is '\n",
            " 'forced to use a small number of binary variables (e.g. 32), the graphical '\n",
            " \"model performs ''semantic hashing'': Documents are mapped to memory \"\n",
            " 'addresses in such a way that semantically similar documents are located at '\n",
            " 'nearby addresses. Documents similar to a query document can then be found by '\n",
            " 'simply accessing all the addresses that differ by only a few bits from the '\n",
            " 'address of the query document. This way of extending the efficiency of '\n",
            " 'hash-coding to approximate matching is much faster than locality sensitive '\n",
            " 'hashing, which is the fastest current method. By using semantic hashing to '\n",
            " 'filter the documents given to TF-IDF, we achieve higher accuracy than '\n",
            " 'applying TF-IDF to the entire document set.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Nonlinear Manifold Embedding on Keyword Spotting Using t-SNE\n",
            "\n",
            "Abstract : \n",
            "('Nonlinear manifold embedding has attracted considerable attention due to its '\n",
            " 'highly-desired property of efficiently encoding local structure, i.e. '\n",
            " 'intrinsic space properties, into a low-dimensional space. The benefit of '\n",
            " 'such an approach is twofold: it leads to compact representations while '\n",
            " 'addressing the often-encountered curse of dimensionality. The latter plays '\n",
            " 'an important role in retrieval applications, such as keyword spotting, where '\n",
            " 'a sorted list of retrieved objects with respect to a distance metric is '\n",
            " 'required. In this work, we explore the efficiency of the popular manifold '\n",
            " 'embedding method t-distributed Stochastic Neighbor Embedding (t-SNE) on the '\n",
            " 'Query-by-Example keyword spotting task. The main contribution of this work '\n",
            " 'is the extension of t-SNE in order to support out-of-sample (OOS) embedding '\n",
            " 'which is essential for mapping query images to the embedding space. The '\n",
            " 'experimental results demonstrate a significant increase in keyword spotting '\n",
            " 'performance when the word similarity is calculated on the embedding space.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/30c0e926bf626e6f300f6a3ae02410509f2fd513\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Jeffreys Centroids: A Closed-Form Expression for Positive Histograms and a Guaranteed Tight Approximation for Frequency Histograms\n",
            "\n",
            "Abstract : \n",
            "('Due to the success of the bag-of-word modeling paradigm, clustering '\n",
            " 'histograms has become an important ingredient of modern information '\n",
            " 'processing. Clustering histograms can be performed using the celebrated '\n",
            " 'k-means centroid-based algorithm. From the viewpoint of applications, it is '\n",
            " 'usually required to deal with symmetric distances. In this letter, we '\n",
            " 'consider the Jeffreys divergence that symmetrizes the Kullback-Leibler '\n",
            " 'divergence, and investigate the computation of Jeffreys centroids. We first '\n",
            " 'prove that the Jeffreys centroid can be expressed analytically using the '\n",
            " 'Lambert W function for positive histograms. We then show how to obtain a '\n",
            " 'fast guaranteed approximation when dealing with frequency histograms. '\n",
            " 'Finally, we conclude with some remarks on the k-means histogram clustering.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/768f868c4e83b05ab7f98b475301042637782ff7\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Accelerating the Divisive Information-Theoretic Clustering of Visual Words\n",
            "\n",
            "Abstract : \n",
            "('Word clustering is an effective approach in the bag- of-words model to '\n",
            " 'reducing the dimensionality of high-dimensional features. In recent years, '\n",
            " 'the bag- of-words model has been successfully introduced into visual '\n",
            " 'recognition and significantly developed. Often, in order to adequately model '\n",
            " 'the complex and diversified visual patterns, a large number of visual words '\n",
            " 'are used, especially in the state-of- the-art visual recognition methods. As '\n",
            " 'a result, the existing word clustering algorithms become not computationally '\n",
            " 'efficient enough. They can considerably prolong the process such as model '\n",
            " 'updating and parameter tuning, where word clustering needs to be repeatedly '\n",
            " 'employed. In this paper, we focus on the divisive information-theoretic '\n",
            " 'clustering, one of the most efficient word clustering algorithms in the '\n",
            " 'field of text analysis, and accelerate its speed to better deal with a large '\n",
            " 'number of visual words. We discuss the properties of its cluster membership '\n",
            " 'evaluation function, KL- divergence, in both binary and multi-class '\n",
            " 'classification cases and develop the accelerated versions in two different '\n",
            " 'ways. Theoretical analysis shows that the proposed accelerated divisive '\n",
            " 'information-theoretic clustering algorithm can handle a large number of '\n",
            " 'visual words in a much more efficient manner. As demonstrated on the '\n",
            " 'benchmark datasets in visual recognition, it can achieve speed-up by '\n",
            " 'hundreds of times while well maintaining the clustering performance of the '\n",
            " 'original algorithm.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/5061abd80bf862552872b4ba3648a578e9b88a3b\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Locally adaptive subspace and similarity metric learning for visual data clustering and retrieval\n",
            "\n",
            "Abstract : \n",
            "('Subspace and similarity metric learning are important issues for image and '\n",
            " 'video analysis in the scenarios of both computer vision and multimedia '\n",
            " 'fields. Many real-world applications, such as image clustering/labeling and '\n",
            " 'video indexing/retrieval, involve feature space dimensionality reduction as '\n",
            " 'well as feature matching metric learning. However, the loss of information '\n",
            " 'from dimensionality reduction may degrade the accuracy of similarity '\n",
            " 'matching. In practice, such basic conflicting requirements for both feature '\n",
            " 'representation efficiency and similarity matching accuracy need to be '\n",
            " \"appropriately addressed. In the style of ''Thinking Globally and Fitting \"\n",
            " \"Locally'', we develop Locally Embedded Analysis (LEA) based solutions for \"\n",
            " 'visual data clustering and retrieval. LEA reveals the essential '\n",
            " 'low-dimensional manifold structure of the data by preserving the local '\n",
            " 'nearest neighbor affinity, and allowing a linear subspace embedding through '\n",
            " 'solving a graph embedded eigenvalue decomposition problem. A visual data '\n",
            " 'clustering algorithm, called Locally Embedded Clustering (LEC), and a local '\n",
            " 'similarity metric learning algorithm for robust video retrieval, called '\n",
            " 'Locally Adaptive Retrieval (LAR), are both designed upon the LEA approach, '\n",
            " 'with variations in local affinity graph modeling. For large size database '\n",
            " 'applications, instead of learning a global metric, we localize the metric '\n",
            " 'learning space with kd-tree partition to localities identified by the '\n",
            " 'indexing process. Simulation results demonstrate the effective performance '\n",
            " 'of proposed solutions in both accuracy and speed aspects.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/349fe62cf4ec9317a6d00d713e8506e257f9400a\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Semantic role-based representations in text classification\n",
            "\n",
            "Abstract : \n",
            "('Although good results for automatic text classification can be achieved with '\n",
            " 'the use of bag-of-words representation, this model is not suitable for all '\n",
            " 'classification problems and richer text representations can be required. In '\n",
            " 'this paper, we proposed two text representation models based on semantic '\n",
            " 'role labels and analyzed them in text classification scenarios. We also '\n",
            " 'evaluated the combination of bag-of-words with a semantic representation '\n",
            " 'considering ensemble multi-view strategies. We explored different '\n",
            " 'classification problems for two text collections and pointed out situations '\n",
            " 'that require more than a bag-of-words. The experimental evaluation indicates '\n",
            " 'that the combination of bag-of-words and a text representation based on '\n",
            " 'semantic role labels can improve text classification accuracies.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/debf00503e1e2fe77ea5e758c94c96e4b4fd8768\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Modeling Stock Prices with Text Contents in 10-Q Reports\n",
            "\n",
            "Abstract : \n",
            "('Stock market prediction was once considered to be infeasible. Recent studies '\n",
            " 'on using text contents of information reporting platforms has opened up new '\n",
            " 'ways of analyzing the stock market with machine learning. we propose using '\n",
            " 'the Securities and Exchange Committee (SEC) mandated 10-Q form as a possible '\n",
            " 'source of data for stock predictions. Using the 10-Q reports of S&P 500 '\n",
            " 'companies, we create our corpus by extracting bag-of-words (BOW) of any '\n",
            " 'additions made to the 10-Q documents. Then, we create feed-forward '\n",
            " 'multilayer neural network on stock price ratios of different target '\n",
            " 'prediction periods and achieve positive prediction rates. We demonstrate '\n",
            " 'that text contents of 10-Q form may have information value to stock price '\n",
            " 'prediction models.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/8a5ecf5e96733288156670ab32491d3f0f412a8c\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Supervised latent semantic indexing for document categorization\n",
            "\n",
            "Abstract : \n",
            "('Latent semantic indexing (LSI) is a successful technology in information '\n",
            " 'retrieval (IR) which attempts to explore the latent semantics implied by a '\n",
            " 'query or a document through representing them in a dimension-reduced space. '\n",
            " 'However, LSI is not optimal for document categorization tasks because it '\n",
            " 'aims to find the most representative features for document representation '\n",
            " 'rather than the most discriminative ones. In this paper, we propose '\n",
            " 'supervised LSI (SLSI) which selects the most discriminative basis vectors '\n",
            " 'using the training data iteratively. The extracted vectors are then used to '\n",
            " 'project the documents into a reduced dimensional space for better '\n",
            " 'classification. Experimental evaluations show that the SLSI approach leads '\n",
            " 'to dramatic dimension reduction while achieving good classification results.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/3ced5e78377b5f4ac6e7c5732d6c57fa6c3cdb4f\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  An adaptive document recognition system for lettrines\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we propose an approach to interactively propagate annotations '\n",
            " 'representing the historians’ knowledge on a database of lettrine images '\n",
            " 'manually populated by historians (with annotations). Based on a novel '\n",
            " 'document indexing processing scheme which combines the use of the Zipf law '\n",
            " 'and the use of bag of patterns, our approach extends the bag-of-words model '\n",
            " 'to represent the knowledge by visual features through relevance feedback. '\n",
            " 'Then, annotation propagation is automatically performed to propagate '\n",
            " 'knowledge to the lettrine database. Our approach is presented together with '\n",
            " 'preliminary experimental results and an illustrative example.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/6d6cfbd6693d16e180c61778762b153b8e4a1d94\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Model-Based Hierarchical Clustering for Categorical Data\n",
            "\n",
            "Abstract : \n",
            "('Agglomerative hierarchical clustering methods based on Gaussian probability '\n",
            " 'models have recently shown to be efficient in different applications. '\n",
            " 'However, the emerging of pattern recognition applications where the features '\n",
            " 'are binary or integer-valued demand extending research efforts to such data '\n",
            " 'types. This paper proposes a hierarchical clustering framework for '\n",
            " 'clustering categorical data based on Multinomial and Bernoulli mixture '\n",
            " 'models. We have compared two widely used density-based distances, namely; '\n",
            " 'Bhattacharyya and KullbackLeibler. The merits of our proposed framework have '\n",
            " 'been shown through extensive experiments on clustering text and images using '\n",
            " 'the bag of visual words model.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/93c8156a4d77fc05ad09764ea812499122472a22\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Object retrieval with large vocabularies and fast spatial matching\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we present a large-scale object retrieval system. The user '\n",
            " 'supplies a query object by selecting a region of a query image, and the '\n",
            " 'system returns a ranked list of images that contain the same object, '\n",
            " 'retrieved from a large corpus. We demonstrate the scalability and '\n",
            " 'performance of our system on a dataset of over 1 million images crawled from '\n",
            " 'the photo-sharing site, Flickr [3], using Oxford landmarks as queries. '\n",
            " 'Building an image-feature vocabulary is a major time and performance '\n",
            " 'bottleneck, due to the size of our dataset. To address this problem we '\n",
            " 'compare different scalable methods for building a vocabulary and introduce a '\n",
            " 'novel quantization method based on randomized trees which we show '\n",
            " 'outperforms the current state-of-the-art on an extensive ground-truth. Our '\n",
            " 'experiments show that the quantization has a major effect on retrieval '\n",
            " 'quality. To further improve query performance, we add an efficient spatial '\n",
            " 'verification stage to re-rank the results returned from our bag-of-words '\n",
            " 'model and show that this consistently improves search quality, though by '\n",
            " 'less of a margin when the visual vocabulary is large. We view this work as a '\n",
            " 'promising step towards much larger, \"web-scale \" image corpora.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/28e4b8ebbdb0e80f03b6f0578deeb38694af081e\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Packing bag-of-features\n",
            "\n",
            "Abstract : \n",
            "('One of the main limitations of image search based on bag-of-features is the '\n",
            " 'memory usage per image. Only a few million images can be handled on a single '\n",
            " 'machine in reasonable response time. In this paper, we first evaluate how '\n",
            " 'the memory usage is reduced by using lossless index compression. We then '\n",
            " 'propose an approximate representation of bag-of-features obtained by '\n",
            " 'projecting the corresponding histogram onto a set of pre-defined sparse '\n",
            " 'projection functions, producing several image descriptors. Coupled with a '\n",
            " 'proper indexing structure, an image is represented by a few hundred bytes. A '\n",
            " 'distance expectation criterion is then used to rank the images. Our method '\n",
            " 'is at least one order of magnitude faster than standard bag-of-features '\n",
            " 'while providing excellent search quality.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/6aae10a6f36a946ddf9d91d101d767694e87f447\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Lost in quantization: Improving particular object retrieval in large scale image databases\n",
            "\n",
            "Abstract : \n",
            "('The state of the art in visual object retrieval from large databases is '\n",
            " 'achieved by systems that are inspired by text retrieval. A key component of '\n",
            " 'these approaches is that local regions of images are characterized using '\n",
            " 'high-dimensional descriptors which are then mapped to ldquovisual wordsrdquo '\n",
            " 'selected from a discrete vocabulary.This paper explores techniques to map '\n",
            " 'each visual region to a weighted set of words, allowing the inclusion of '\n",
            " 'features which were lost in the quantization stage of previous systems. The '\n",
            " 'set of visual words is obtained by selecting words based on proximity in '\n",
            " 'descriptor space. We describe how this representation may be incorporated '\n",
            " 'into a standard tf-idf architecture, and how spatial verification is '\n",
            " 'modified in the case of this soft-assignment. We evaluate our method on the '\n",
            " 'standard Oxford Buildings dataset, and introduce a new dataset for '\n",
            " 'evaluation. Our results exceed the current state of the art retrieval '\n",
            " 'performance on these datasets, particularly on queries with poor initial '\n",
            " 'recall where techniques like query expansion suffer. Overall we show that '\n",
            " 'soft-assignment is always beneficial for retrieval with large vocabularies, '\n",
            " 'at a cost of increased storage requirements for the index.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/b6371f8c70c2684faefd99fffcc556c3a75dd7f4\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Small codes and large image databases for recognition\n",
            "\n",
            "Abstract : \n",
            "('The Internet contains billions of images, freely available online. Methods '\n",
            " 'for efficiently searching this incredibly rich resource are vital for a '\n",
            " 'large number of applications. These include object recognition, computer '\n",
            " 'graphics, personal photo collections, online image search tools. In this '\n",
            " 'paper, our goal is to develop efficient image search and scene matching '\n",
            " 'techniques that are not only fast, but also require very little memory, '\n",
            " 'enabling their use on standard hardware or even on handheld devices. Our '\n",
            " 'approach uses recently developed machine learning techniques to convert the '\n",
            " 'Gist descriptor (a real valued vector that describes orientation energies at '\n",
            " 'different scales and orientations within an image) to a compact binary code, '\n",
            " 'with a few hundred bits per image. Using our scheme, it is possible to '\n",
            " 'perform real-time searches with millions from the Internet using a single '\n",
            " 'large PC and obtain recognition results comparable to the full descriptor. '\n",
            " 'Using our codes on high quality labeled images from the LabelMe database '\n",
            " 'gives surprisingly powerful recognition results using simple nearest '\n",
            " 'neighbor techniques.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/993cb3c7167b34b033c66cc4b2af87ff4781c6d4\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Large-scale image retrieval with compressed Fisher vectors\n",
            "\n",
            "Abstract : \n",
            "('The problem of large-scale image search has been traditionally addressed '\n",
            " 'with the bag-of-visual-words (BOV). In this article, we propose to use as an '\n",
            " 'alternative the Fisher kernel framework. We first show why the Fisher '\n",
            " 'representation is well-suited to the retrieval problem: it describes an '\n",
            " 'image by what makes it different from other images. One drawback of the '\n",
            " 'Fisher vector is that it is high-dimensional and, as opposed to the BOV, it '\n",
            " 'is dense. The resulting memory and computational costs do not make Fisher '\n",
            " 'vectors directly amenable to large-scale retrieval. Therefore, we compress '\n",
            " 'Fisher vectors to reduce their memory footprint and speed-up the retrieval. '\n",
            " 'We compare three binarization approaches: a simple approach devised for this '\n",
            " 'representation and two standard compression techniques. We show on two '\n",
            " 'publicly available datasets that compressed Fisher vectors perform very well '\n",
            " 'using as little as a few hundreds of bits per image, and significantly '\n",
            " 'better than a very recent compressed BOV approach.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/48257a889a9aa61998ae20fa52b25d90c441f63a\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  An effective object recognition system by a mobile application\n",
            "\n",
            "Abstract : \n",
            "('Unparalleled growth in the sharing of media via networks has prompted a '\n",
            " 'great deal of research into issues pertaining to image retrieval. The '\n",
            " 'training and verification of image retrieval systems requires a large number '\n",
            " 'of labelled images with ground truth; however, most researchers employ '\n",
            " 'synthetic datasets for their experiments. In this study, we developed a '\n",
            " 'system based on a mobile phone App for the collection of information '\n",
            " 'pertaining to the location of objects in images. The proposed system is '\n",
            " 'simple and easy to use. Experiments demonstrate the excellent performance of '\n",
            " 'the proposed system with regard to accuracy and response time. This study '\n",
            " 'demonstrates the feasibility of collecting image information using mobile '\n",
            " 'phones.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/5cb08503f44f8e0aaeb7bd18faaba71022b8934a\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Learning to Combine Ad-hoc Ranking Functions for Image Retrieval\n",
            "\n",
            "Abstract : \n",
            "('Along with the success of \"bag of visual words\" scheme in content based '\n",
            " 'image retrieval (CBIR), various technologies in text information retrieval '\n",
            " 'realm have been transferred into image retrieval system and obtain promising '\n",
            " 'performance. However, how to select the suitable ranking technology, such as '\n",
            " 'ranking model, for a specific image database is still an open question. '\n",
            " 'Because most ranking models are data-dependent, it is hard to find an '\n",
            " 'optimal model for all the applications. In this paper, we propose to resolve '\n",
            " 'this problem for CBIR with the learning to rank approach which has been '\n",
            " 'widely utilized in text retrieval. Specifically, we consider several well '\n",
            " 'performed ad-hoc ranking models and use their ranking scores to construct '\n",
            " 'the ranking features for the Ranking SVM framework. To best preserve the '\n",
            " 'spatial structures existed in the visual words of image, we split the image '\n",
            " 'into different size blocks, and design the ranking features with a pyramid '\n",
            " 'approach from large blocks to small blocks. Experimental results on both '\n",
            " 'Oxford and Image Net databases demonstrate the effectiveness of proposed '\n",
            " 'method compared with the performance that individual ranking model is '\n",
            " 'adopted. Moreover, the proposed method brings little computational burden to '\n",
            " 'the system and the efficiency analysis proves its scalability.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/b861f715776aacf31e6b0a3bdbda54dda1195f51\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Semi-supervised Discriminant Hashing\n",
            "\n",
            "Abstract : \n",
            "('Hashing refers to methods for embedding high dimensional data into a '\n",
            " 'similarity-preserving low-dimensional Hamming space such that similar '\n",
            " 'objects are indexed by binary codes whose Hamming distances are small. '\n",
            " 'Learning hash functions from data has recently been recognized as a '\n",
            " 'promising approach to approximate nearest neighbor search for high '\n",
            " \"dimensional data. Most of ¡®learning to hash' methods resort to either \"\n",
            " 'unsupervised or supervised learning to determine hash functions. Recently '\n",
            " 'semi-supervised learning approach was introduced in hashing where pair wise '\n",
            " 'constraints (must link and cannot-link) using labeled data are leveraged '\n",
            " 'while unlabeled data are used for regularization to avoid over-fitting. In '\n",
            " 'this paper we base our semi-supervised hashing on linear discriminant '\n",
            " 'analysis, where hash functions are learned such that labeled data are used '\n",
            " 'to maximize the separability between binary codes associated with different '\n",
            " 'classes while unlabeled data are used for regularization as well as for '\n",
            " 'balancing condition and pair wise decor relation of bits. The resulting '\n",
            " 'method is referred to as semi-supervised discriminant hashing (SSDH). '\n",
            " 'Numerical experiments on MNIST and CIFAR-10 datasets demonstrate that our '\n",
            " 'method outperforms existing methods, especially in the case of short binary '\n",
            " 'codes.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/794d506a2ad89b89ae741773011bc1de3b62bc51\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  A Comprehensive Study Over VLAD and Product Quantization in Large-Scale Image Retrieval\n",
            "\n",
            "Abstract : \n",
            "('This paper deals with content-based large-scale image retrieval using the '\n",
            " 'state-of-the-art framework of VLAD and Product Quantization proposed by '\n",
            " 'Jegou as a starting point. Demonstrating an excellent accuracy-efficiency '\n",
            " 'trade-off, this framework has attracted increased attention from the '\n",
            " 'community and numerous extensions have been proposed. In this work, we make '\n",
            " 'an in-depth analysis of the framework that aims at increasing our '\n",
            " 'understanding of its different processing steps and boosting its overall '\n",
            " 'performance. Our analysis involves the evaluation of numerous extensions '\n",
            " '(both existing and novel) as well as the study of the effects of several '\n",
            " 'unexplored parameters. We specifically focus on: a) employing more efficient '\n",
            " 'and discriminative local features; b) improving the quality of the '\n",
            " 'aggregated representation; and c) optimizing the indexing scheme. Our '\n",
            " 'thorough experimental evaluation provides new insights into extensions that '\n",
            " 'consistently contribute, and others that do not, to performance improvement, '\n",
            " 'and sheds light onto the effects of previously unexplored parameters of the '\n",
            " 'framework. As a result, we develop an enhanced framework that significantly '\n",
            " 'outperforms the previous best reported accuracy results on standard '\n",
            " 'benchmarks and is more efficient.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/f389e66bcc210d50bc5ea8f90f851e1f3e281354\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Residual Enhanced Visual Vectors for on-device image matching\n",
            "\n",
            "Abstract : \n",
            "('Most mobile visual search (MVS) systems query a large database stored on a '\n",
            " 'server. This paper presents a new architecture for searching a large '\n",
            " 'database directly on a mobile device, which has numerous benefits for '\n",
            " 'network-independent, low-latency, and privacy-protected image retrieval. A '\n",
            " 'key challenge for on-device MVS is storing a memory-intensive database in '\n",
            " 'the limited RAM of the mobile device. We design and implement a new compact '\n",
            " 'global image signature called the Residual Enhanced Visual Vector (REVV) '\n",
            " 'that is optimized for the local features typically used in MVS. REVV '\n",
            " 'outperforms existing compact database representations in the MVS setting and '\n",
            " 'attains similar retrieval accuracy in large-scale retrieval tests as a '\n",
            " 'Vocabulary Tree that uses 26× more memory. The compactness of REVV '\n",
            " 'consequently enables many database images to be queried on a mobile device.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/7f0c7df799cd1b75ca7e1fc248b5c1377fb18dad\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Deep Learning to Hash with Multiple Representations\n",
            "\n",
            "Abstract : \n",
            "('Hashing seeks an embedding of high-dimensional objects into a '\n",
            " 'similarity-preserving low-dimensional Hamming space such that similar '\n",
            " 'objects are indexed by binary codes with small Hamming distances. A variety '\n",
            " 'of hashing methods have been developed, but most of them resort to a single '\n",
            " 'view (representation) of data. However, objects are often described by '\n",
            " 'multiple representations. For instance, images are described by a few '\n",
            " 'different visual descriptors (such as SIFT, GIST, and HOG), so it is '\n",
            " 'desirable to incorporate multiple representations into hashing, leading to '\n",
            " 'multi-view hashing. In this paper we present a deep network for multi-view '\n",
            " 'hashing, referred to as deep multi-view hashing, where each layer of hidden '\n",
            " 'nodes is composed of view-specific and shared hidden nodes, in order to '\n",
            " 'learn individual and shared hidden spaces from multiple views of data. '\n",
            " 'Numerical experiments on image datasets demonstrate the useful behavior of '\n",
            " 'our deep multi-view hashing (DMVH), compared to recently-proposed '\n",
            " 'multi-modal deep network as well as existing shallow models of hashing.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/908f505d8fe4f55f24994af4c7362e0959bfe4f7\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  MFC: A multi-scale fully convolutional approach for visual instance retrieval\n",
            "\n",
            "Abstract : \n",
            "('Previous work has shown that feature maps of deep convolutional neural '\n",
            " 'networks (CNNs) can be interpreted as feature representation of an image. '\n",
            " 'Image features aggregated from these feature maps have achieved steady '\n",
            " 'progress in terms of performances on visual instance retrieval tasks in '\n",
            " 'recent years. The key to the success of such methods is feature '\n",
            " 'representation. In this paper, we study how to represent an image using '\n",
            " 'discriminative features. We demonstrate first that image size is an '\n",
            " 'important factor which affects the performance of instance retrieval but has '\n",
            " 'not been thoroughly discussed in previous work. Based on experimental '\n",
            " 'evaluations, we propose a multi-scale fully convolutional (MFC) approach to '\n",
            " 'encode the image efficiently and effectively. The proposed method is simple '\n",
            " 'to implement, which does not employ sophisticated post-processing techniques '\n",
            " 'such as query expansion, yet shows promising results on four public '\n",
            " 'datasets.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/8734a94034b9f118c3cefc4a1d785ec94b3a226e\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Optimised KD-trees for fast image descriptor matching\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we look at improving the KD-tree for a specific usage: '\n",
            " 'indexing a large number of SIFT and other types of image descriptors. We '\n",
            " 'have extended priority search, to priority search among multiple trees. By '\n",
            " 'creating multiple KD-trees from the same data set and simultaneously '\n",
            " 'searching among these trees, we have improved the KD-treepsilas search '\n",
            " 'performance significantly.We have also exploited the structure in SIFT '\n",
            " 'descriptors (or structure in any data set) to reduce the time spent in '\n",
            " 'backtracking. By using Principal Component Analysis to align the principal '\n",
            " 'axes of the data with the coordinate axes, we have further increased the '\n",
            " 'KD-treepsilas search performance.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/09d947e7cd215abf5d45463b64859ddc0f7e1a8e\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Exploring intra-BOW statistics for improving visual categorization\n",
            "\n",
            "Abstract : \n",
            "('Research in video retrieval systems is mainly inspired by the state of the '\n",
            " 'art text retrieval where high dimensional descriptors are quantized to '\n",
            " 'visual words making a Bag Of Words (BOW) histogram for an image. For a small '\n",
            " 'BOW model potentially different descriptors could get assigned to the same '\n",
            " 'visual word. Recently however refinements have been proposed to recover some '\n",
            " 'of this representation loss for this simplistic model of visual description '\n",
            " 'by studying the distribution of descriptors within the visual words [1, 2, '\n",
            " '3]. Following the same foot-steps we enhance the BOW by encoding the '\n",
            " 'position of each of the descriptor inside the quantized cell according to '\n",
            " 'its centroid. Embedding this information to represent images increases '\n",
            " 'precision of video concept detection. We compare our method to a BOW based '\n",
            " 'baseline on TRECVID 2007 and TRECVID 2010 [4] datasets and show that adding '\n",
            " 'the refinement proposed always improves the semantic indexing task. We also '\n",
            " 'compare our method to that of [3] and show that it outperforms the Hamming '\n",
            " 'Embedding Similarity based classification on the TRECVID 2007 dataset and '\n",
            " 'illustrates comparable performance on the TRECVID 2010 set.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/093644b0698378551bb31fdf69c01cb08138abaf\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Query-Adaptive Image Search With Hash Codes\n",
            "\n",
            "Abstract : \n",
            "('Scalable image search based on visual similarity has been an active topic of '\n",
            " 'research in recent years. State-of-the-art solutions often use hashing '\n",
            " 'methods to embed high-dimensional image features into Hamming space, where '\n",
            " 'search can be performed in real-time based on Hamming distance of compact '\n",
            " 'hash codes. Unlike traditional metrics (e.g., Euclidean) that offer '\n",
            " 'continuous distances, the Hamming distances are discrete integer values. As '\n",
            " 'a consequence, there are often a large number of images sharing equal '\n",
            " 'Hamming distances to a query, which largely hurts search results where '\n",
            " 'fine-grained ranking is very important. This paper introduces an approach '\n",
            " 'that enables query-adaptive ranking of the returned images with equal '\n",
            " 'Hamming distances to the queries. This is achieved by firstly offline '\n",
            " 'learning bitwise weights of the hash codes for a diverse set of predefined '\n",
            " 'semantic concept classes. We formulate the weight learning process as a '\n",
            " 'quadratic programming problem that minimizes intra-class distance while '\n",
            " 'preserving inter-class relationship captured by original raw image features. '\n",
            " 'Query-adaptive weights are then computed online by evaluating the proximity '\n",
            " 'between a query and the semantic concept classes. With the query-adaptive '\n",
            " 'bitwise weights, returned images can be easily ordered by weighted Hamming '\n",
            " 'distance at a finer-grained hash code level rather than the original Hamming '\n",
            " 'distance level. Experiments on a Flickr image dataset show clear '\n",
            " 'improvements from our proposed approach.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/d8ddd314ee02df68bc8c9e1130bbbb0985af676b\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Learning to Distribute Vocabulary Indexing for Scalable Visual Search\n",
            "\n",
            "Abstract : \n",
            "('In recent years, there is an ever-increasing research focus on Bag-of-Words '\n",
            " 'based near duplicate visual search paradigm with inverted indexing. One '\n",
            " 'fundamental yet unexploited challenge is how to maintain the large indexing '\n",
            " 'structures within a single server subject to its memory constraint, which is '\n",
            " 'extremely hard to scale up to millions or even billions of images. In this '\n",
            " 'paper, we propose to parallelize the near duplicate visual search '\n",
            " 'architecture to index millions of images over multiple servers, including '\n",
            " 'the distribution of both visual vocabulary and the corresponding indexing '\n",
            " 'structure. We optimize the distribution of vocabulary indexing from a '\n",
            " 'machine learning perspective, which provides a “memory light” search '\n",
            " 'paradigm that leverages the computational power across multiple servers to '\n",
            " 'reduce the search latency. Especially, our solution addresses two essential '\n",
            " 'issues: “What to distribute” and “How to distribute”. “What to distribute” '\n",
            " 'is addressed by a “lossy” vocabulary Boosting, which discards both frequent '\n",
            " 'and indiscriminating words prior to distribution. “How to distribute” is '\n",
            " 'addressed by learning an optimal distribution function, which maximizes the '\n",
            " 'uniformity of assigning the words of a given query to multiple servers. We '\n",
            " 'validate the distributed vocabulary indexing scheme in a real world location '\n",
            " 'search system over 10 million landmark images. Comparing to the '\n",
            " 'state-of-the-art alternatives of single-server search [5], [6], [16] and '\n",
            " 'distributed search [23], our scheme has yielded a significant gain of about '\n",
            " '200% speedup at comparable precision by distributing only 5% words. We also '\n",
            " 'report excellent robustness even when partial servers crash.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/de51dafe74457eeb553bb2de99ad12f874c0b5c7\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  DFlow and DField: New features for capturing object and image relationships\n",
            "\n",
            "Abstract : \n",
            "('In this paper we propose two new types of features useful for problems in '\n",
            " 'which one wants to describe object or image relationships rather than '\n",
            " 'objects or images themselves. The features are based on the notion of '\n",
            " 'distribution flow, as derived from the classic Transportation Problem. Two '\n",
            " 'variants of such features, the Distribution Flow (DFlow) and Displacement '\n",
            " 'Field (DField), are defined and studied. The proposed features show '\n",
            " 'promising results in two different applications, Inter- and Intra-Class '\n",
            " 'Relationship Characterization, and improve on simple concatenation of '\n",
            " 'corresponding pairs of histograms.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/77fee197fea03e5cfd9d0ecc55ddc777248955d5\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Mining Multilevel Image Semantics via Hierarchical Classification\n",
            "\n",
            "Abstract : \n",
            "('In this paper, we have proposed a novel framework for mining multilevel '\n",
            " 'image semantics via hierarchical classification. To bridge the semantic gap '\n",
            " 'more successfully, salient objects are used to characterize the intermediate '\n",
            " 'image semantics effectively. The salient objects are defined as the '\n",
            " 'connected image regions that capture the dominant visual properties linked '\n",
            " 'to the corresponding physical objects in an image. To achieve a more '\n",
            " 'reliable and tractable concept learning in high-dimensional feature space, a '\n",
            " 'novel algorithm called product of mixture-experts (PoM) is proposed to '\n",
            " 'reduce the size of training images and speed up concept learning. A novel '\n",
            " 'hierarchical concept learning algorithm is proposed by incorporating concept '\n",
            " 'ontology and multitask learning to enhance the discrimination power of the '\n",
            " 'concept models and reduce the computational complexity for learning the '\n",
            " 'concept models for large amount of image concepts, which may have huge '\n",
            " 'intra-concept variations and inter-concept similarities on their visual '\n",
            " 'properties. A hyperbolic image visualization algorithm has been developed '\n",
            " 'for allowing users to specify their queries easily and assess the query '\n",
            " 'results interactively. Our experiments on large-scale image collections have '\n",
            " 'also obtained very positive results.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/8dff34cf73398b5b41d237cd7f18452138929473\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Compressing Deep Neural Networks for Recognizing Places\n",
            "\n",
            "Abstract : \n",
            "('Visual place recognition on low memory devices such as mobile phones and '\n",
            " 'robotics systems is a challenging problem. The state of the art models for '\n",
            " 'this task uses deep learning architectures having close to 100 million '\n",
            " 'parameters which take over 400MB of memory. This makes these models '\n",
            " 'infeasible to be deployed in low memory devices and gives rise to the need '\n",
            " 'of compressing them. Hence we study the effectiveness of model compression '\n",
            " 'techniques like trained quantization and pruning for reducing the number of '\n",
            " 'parameters on one of the best performing image retrieval models called '\n",
            " 'NetVLAD. We show that a compressed network can be created by starting with a '\n",
            " 'model pre-trained for the task of visual place recognition and then '\n",
            " 'fine-tuning it via trained pruning and quantization. The compressed model is '\n",
            " 'able to produce the same mAP as the original uncompressed network. We '\n",
            " 'achieve almost 50% parameter pruning with no loss in mAP and 70% pruning '\n",
            " 'with close to 2% mAP reduction, while also performing 8-bit quantization. '\n",
            " 'Furthermore, together with 5-bit quantization, we perform about 50% '\n",
            " 'parameter reduction by pruning and get only about 3% reduction in mAP. The '\n",
            " 'resulting compressed networks have sizes of around 30MB and 65MB which makes '\n",
            " 'them easily usable in memory constrained devices.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/a5fe8fc8d5a675a59fb3814cf5009655583b805c\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Learning Hierarchical Semantic Description Via Mixed-Norm Regularization for Image Understanding\n",
            "\n",
            "Abstract : \n",
            "('This paper proposes a new perspective-Vicept representation to solve the '\n",
            " 'problem of visual polysemia and concept polymorphism in the large-scale '\n",
            " 'semantic image understanding. Vicept characterizes the membership '\n",
            " 'probability distribution between visual appearances and semantic concepts, '\n",
            " 'and forms a hierarchical representation of image semantic from local to '\n",
            " 'global. In the implementation, incorporating group sparse coding, visual '\n",
            " 'appearance is encoded as a weighted sum of dictionary elements, which could '\n",
            " 'obtain more accurate image representation with sparsity at the image level. '\n",
            " 'To obtain discriminative Vicept descriptions with structural sparsity, '\n",
            " 'mixed-norm regularization is adopted in the optimization problem for '\n",
            " 'learning the concept membership distribution of visual appearance. '\n",
            " 'Furthermore, we introduce a novel image distance measurement based on the '\n",
            " 'hierarchical Vicept description, where different levels of Vicept distance '\n",
            " 'are fused together by multi-level separability analysis. Finally, the wide '\n",
            " 'applications of Vicept description are validated in our experiments, '\n",
            " 'including large-scale semantic image search, image annotation, and semantic '\n",
            " 'image re-ranking.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/144dc69bcda9ad87fed6c50a8727062123b4e838\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Dual local consistency hashing with discriminative projections selection\n",
            "\n",
            "Abstract : \n",
            "('Semantic hashing is a promising way to accelerate similarity search, which '\n",
            " 'designs compact binary codes for a large number of images so that '\n",
            " 'semantically similar images are mapped to close codes. Retrieving similar '\n",
            " 'neighbors is then simply accomplished by retrieving images that have codes '\n",
            " 'within a small Hamming distance of the code of the query. However, most of '\n",
            " 'the existing hashing approaches, such as spectral hashing (SH), learn the '\n",
            " 'binary codes by preserving the global similarity, which do not have full '\n",
            " 'discriminative power. In this paper, we propose a dual local consistency '\n",
            " 'hashing method which not only makes the similar images have the same codes '\n",
            " 'but also dissimilar images with different codes. Moreover, we propose a PCA '\n",
            " 'projection selecting scheme that choose the most discriminative projection '\n",
            " 'for each bit of the codes. Therefore, the binary codes learned by our '\n",
            " 'approach are more powerful and discriminative for similarity search. '\n",
            " 'Extensive experiments are conducted on publicly available datasets and the '\n",
            " 'comparison results demonstrate that our approach can outperform the '\n",
            " 'state-of-art methods.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/633f7b63f33a24dde509edf60f92e4b4cbe89b75\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Large-Scale Video Retrieval Using Image Queries\n",
            "\n",
            "Abstract : \n",
            "('Retrieving videos from large repositories using image queries is important '\n",
            " 'for many applications, such as brand monitoring or content linking. We '\n",
            " 'introduce a new retrieval architecture, in which the image query can be '\n",
            " 'compared directly with database videos—significantly improving retrieval '\n",
            " 'scalability compared with a baseline system that searches the database on a '\n",
            " 'video frame level. Matching an image to a video is an inherently asymmetric '\n",
            " 'problem. We propose an asymmetric comparison technique for Fisher vectors '\n",
            " 'and systematically explore query or database items with varying amounts of '\n",
            " 'clutter, showing the benefits of the proposed technique. We then propose '\n",
            " 'novel video descriptors that can be compared directly with image '\n",
            " 'descriptors. We start by constructing Fisher vectors for video segments, by '\n",
            " 'exploring different aggregation techniques. For a database of lecture '\n",
            " 'videos, such methods obtain a two orders of magnitude compression gain with '\n",
            " 'respect to a frame-based scheme, with no loss in retrieval accuracy. Then, '\n",
            " 'we consider the design of video descriptors, which combine Fisher embedding '\n",
            " 'with hashing techniques, in a flexible framework based on Bloom filters. '\n",
            " 'Large-scale experiments using three datasets show that this technique '\n",
            " 'enables faster and more memory-efficient retrieval, compared with a '\n",
            " 'frame-based method, with similar accuracy. The proposed techniques are '\n",
            " 'further compared against pre-trained convolutional neural network features, '\n",
            " 'outperforming them on three datasets by a substantial margin.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/50f1e841cf35c9898bc1497a90a70803123cd20a\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Image Retrieval with a Visual Thesaurus\n",
            "\n",
            "Abstract : \n",
            "('Current state-of-art of image retrieval methods represent images as an '\n",
            " 'unordered collection of local patches, each of which is classified as a '\n",
            " '\"visual word\" from a fixed vocabulary. This paper presents a simple but '\n",
            " 'innovative way to uncover the spatial relationship between visual words so '\n",
            " 'that we can discover words that represent the same latent topic and thereby '\n",
            " 'improve the retrieval results. The method in this paper is borrowed from '\n",
            " 'text retrieval, and is analogous to a text thesaurus in that it describes a '\n",
            " 'broad set of equivalence relationship between words. We evaluate our method '\n",
            " 'on the popular Oxford Building dataset. This makes it possible to compare '\n",
            " 'our method with previous work on image retrieval, and the results show that '\n",
            " 'our method is comparable to more complex state of the art methods.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/158ad930d71cf72539deb1baeeca21dcc85393a9\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  AVLAD: Optimizing the VLAD Image Signature for Specific Feature Descriptors\n",
            "\n",
            "Abstract : \n",
            "('Recent works on content-based image retrieval have successfully used the '\n",
            " 'Vector of Locally Aggregated Descriptors (VLAD) as a compact image '\n",
            " 'signature. In this paper, we improve the VLAD signature by tailoring the '\n",
            " 'VLAD representation to the specific properties of the visual feature '\n",
            " 'descriptors. We combine this improvement with the recently proposed '\n",
            " 'hierarchical VLAD approach and demonstrate the effectiveness of this '\n",
            " 'extension for the two well-known feature descriptors SIFT and SURF. '\n",
            " 'Furthermore, we investigate how to efficiently reduce the dimensionality of '\n",
            " 'the resulting representation using different unsupervised dimensionality '\n",
            " 'reduction techniques.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/0a62942faef527b159d4580cec68086f5e3ce3de\n",
            "---\n",
            "-------\n",
            "---------------\n",
            "-------\n",
            "---\n",
            "Title:  Semi-supervised hashing for scalable image retrieval\n",
            "\n",
            "Abstract : \n",
            "('Large scale image search has recently attracted considerable attention due '\n",
            " 'to easy availability of huge amounts of data. Several hashing methods have '\n",
            " 'been proposed to allow approximate but highly efficient search. Unsupervised '\n",
            " 'hashing methods show good performance with metric distances but, in image '\n",
            " 'search, semantic similarity is usually given in terms of labeled pairs of '\n",
            " 'images. There exist supervised hashing methods that can handle such semantic '\n",
            " 'similarity but they are prone to overfitting when labeled data is small or '\n",
            " 'noisy. Moreover, these methods are usually very slow to train. In this work, '\n",
            " 'we propose a semi-supervised hashing method that is formulated as minimizing '\n",
            " 'empirical error on the labeled data while maximizing variance and '\n",
            " 'independence of hash bits over the labeled and unlabeled data. The proposed '\n",
            " 'method can handle both metric as well as semantic similarity. The '\n",
            " 'experimental results on two large datasets (up to one million samples) '\n",
            " 'demonstrate its superior performance over state-of-the-art supervised and '\n",
            " 'unsupervised methods.')\n",
            "\n",
            "Link: https://www.semanticscholar.org/paper/bb963c63ee55eab58371bc4ba0690f3eb0f498a8\n",
            "---\n",
            "-------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiNISoSa2xXf",
        "colab_type": "text"
      },
      "source": [
        "## If you have any additional feedback about a query, or just feedback in general, we would very much appreciate it. The feedback will help in the qualitative analysis of our models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJcKvcK4zL6G",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Feedback about a particular query\n",
        "\n",
        "%%capture\n",
        "\n",
        "query = \"Vector representation of text for information retrieval. Document embeddings for search. Vector representation of query. Embedding representation of queries. \" #@param {type:\"string\"}\n",
        "\n",
        "feedback = \"First result didn't seem to say anything about negative sampling\" #@param {type:\"string\"}\n",
        "\n",
        "blockPrint()\n",
        "values_list = worksheet2.col_values(3)\n",
        "values_list2 = worksheet2.col_values(4)\n",
        "rowV = max(len(values_list) , len(values_list2) )\n",
        "worksheet2.update_cell(rowV+1, 3, query)\n",
        "worksheet2.update_cell(rowV+1, 4, feedback)\n",
        "enablePrint()\n",
        "\n",
        "print('Submitted')\n",
        "print('Query recorded, ', query)\n",
        "print('Feedback recorded, ', feedback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc3PMILi2LN6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Feedback\n",
        "\n",
        "%%capture\n",
        "\n",
        "feedback = \"UI could use some work\" #@param {type:\"string\"}\n",
        "\n",
        "blockPrint()\n",
        "values_list = worksheet3.col_values(3)\n",
        "values_list2 = worksheet3.col_values(4)\n",
        "rowV = max(len(values_list) , len(values_list2) )\n",
        "worksheet3.update_cell(rowV+1, 3, feedback)\n",
        "enablePrint()\n",
        "\n",
        "print('Submitted')\n",
        "print('Feedback recorded, ', feedback)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}