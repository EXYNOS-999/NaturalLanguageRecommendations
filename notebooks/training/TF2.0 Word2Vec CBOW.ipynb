{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2.0 Word2Vec CBOW",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9weW9X7-vLsg",
        "colab_type": "text"
      },
      "source": [
        "Although in the end this model was not used, our group felt it'd still be appropriate to add to our github for the completeness of our submission to TFWorld competition and to allow the community to use this model if the need arose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gFQvB6MatHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, dot, Reshape, Embedding\n",
        "# from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDVgocMrv0lm",
        "colab_type": "text"
      },
      "source": [
        "A note for the following cell. Althouhg our model is the continuous bag of words version of Word2Vec we used Keras's skipgrams preprocessing for nengative sampling due to how our data wasn't formated as sequences. Read more on these functions [here](https://keras.io/preprocessing/sequence/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP91bFU8Qq6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
        "def sampling(inCitation:list, outCitation:list, window_size=3):\n",
        "    ''' InCitation the original paper, OutCitation: the papers cited in the original paper'''\n",
        "    global vocab_size\n",
        "    vocab = list(set(inCitation))\n",
        "    for out in outCitation:\n",
        "      for paper in out:\n",
        "        vocab.append(paper)\n",
        "    vocab = list(set(vocab))\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    sampling_table = make_sampling_table(vocab_size)\n",
        "    labels = []\n",
        "    data = []\n",
        "    target = []\n",
        "    for i in range(len(inCitation)):\n",
        "        out = outCitation[i] \n",
        "        contexes, label = skipgrams(out, vocab_size, window_size=window_size)\n",
        "        data.append(contexes)\n",
        "        labels.append(label)\n",
        "        target.append([inCitation[i]] * len(label))\n",
        "    \n",
        "    return target, data, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7EOtljpdQES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target, data, labels = sampling([1,2,3,5,6], [[3,6,4], [4,2,1], [1,2,5], [2,5,3], [1,2,3], [5]]) #Dummy data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZuv1sHSjWKo",
        "colab_type": "code",
        "outputId": "d7b77197-f7a0-47dc-b31d-d249fc148df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class Word2CBOW(keras.Model): #I should stop naming things\n",
        "  def __init__(self, window_size=3, **kwargs):\n",
        "    super().__init__(**kwargs) #handles standard args (e.g., name)\n",
        "    #super() is to use the keras.Model class\n",
        "    #To add: Argument for window size and arg for\n",
        "    self.embedding_layer = Embedding(7, 768, input_length=2) #Only working with a vocab of 6 to test\n",
        "    self.window_size = 3\n",
        "    self.context_window = self.window_size * 2\n",
        "    self.outvec = Dense(1, activation='sigmoid')\n",
        "    self.similarity = 0\n",
        "    self.cbow = Lambda(lambda x: K.mean(x, axis=[-1]))\n",
        "    self.batched_dot = Lambda(self.bdotFunction)\n",
        "\n",
        "  def bdotFunction(self, x):\n",
        "        first = x[0]\n",
        "        second = x[1]\n",
        "        return K.batch_dot(first, second, axes=-1)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    target_input, context_inputs = inputs\n",
        "\n",
        "    target_input = keras.layers.InputLayer(input_shape=[1,])(target_input)\n",
        "    context_inputs = keras.layers.InputLayer(input_shape=[2,])(context_inputs)\n",
        "    \n",
        "    target1 = self.embedding_layer(target_input)\n",
        "    context = self.embedding_layer(context_inputs)\n",
        "   \n",
        "    context = self.cbow(context) #Averaging the context vectors\n",
        "    \n",
        "\n",
        "    dotted = self.batched_dot([tf.squeeze(target1, axis=0), tf.squeeze(context, axis=0)])\n",
        "\n",
        "    binary_output = self.outvec(dotted)\n",
        "    binary_output = tf.squeeze(binary_output)\n",
        "\n",
        "    return binary_output\n",
        "model = Word2CBOW()\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "# labels = np.array(np.transpose(labels))\n",
        "model.fit((np.array(target, dtype=np.int32), np.array(data, dtype=np.int32)), y=np.array(labels), epochs=100, batch_size=1)\n",
        "# model((np.array(target[0], dtype=np.int32), np.array(data[0], dtype=np.int32)))\n",
        "#Might have to try train_on_batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12,)\n",
            "(12,)\n",
            "Train on 5 samples\n",
            "Epoch 1/100\n",
            "(12,)\n",
            "(12,)\n",
            "(12,)\n",
            "(12,)\n",
            "5/5 [==============================] - 1s 113ms/sample - loss: 0.6924\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6896\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6876\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6856\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6835\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6815\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6793\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6771\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6747\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6723\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6699\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6676\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6651\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6627\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6604\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6582\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6560\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6538\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6515\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6492\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6471\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6450\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 6ms/sample - loss: 0.6428\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6406\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6384\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6364\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6343\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6321\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6301\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6282\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6263\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6244\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6226\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6207\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6190\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6174\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6158\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6143\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6129\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6113\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6100\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6086\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6073\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6060\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6048\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6037\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6025\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.6013\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.6002\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5990\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5980\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5971\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5961\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5951\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5943\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5934\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5925\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5917\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5910\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5901\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5895\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5888\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5882\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5876\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5870\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5864\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5858\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5854\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5849\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5845\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5840\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5836\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5831\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5827\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5824\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5820\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5816\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5813\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5809\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5807\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5803\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5801\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5797\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5794\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5792\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5789\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 3ms/sample - loss: 0.5787\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5784\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5781\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5778\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5776\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5774\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5772\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5769\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5767\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5765\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5763\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5761\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 5ms/sample - loss: 0.5758\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 4ms/sample - loss: 0.5755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fae939566d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 444
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faQEWPbSxnQ7",
        "colab_type": "code",
        "outputId": "9f2848c0-396f-41c2-c643-6f4393f2471e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(np.transpose(labels)).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 409
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4a0gK6MOpu3",
        "colab_type": "code",
        "outputId": "90f3451d-3ef0-4059-a592-a3f823ee6b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(labels).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 400
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVrmlR4wdNa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# target = [1,2,3,5,6]\n",
        "inputs = (np.array(target[0], dtype=np.int32), np.array(data[0], dtype=np.int32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqEk4_HadQ9k",
        "colab_type": "code",
        "outputId": "853fb5ef-d3d7-4373-ee45-342815832b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "target_input, context_inputs = inputs\n",
        "target_input = keras.layers.InputLayer(input_shape=[1,])(target_input)\n",
        "context_inputs = keras.layers.InputLayer(input_shape=[2,])(context_inputs)\n",
        "context_inputs.shape, target_input.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([12, 2]), TensorShape([12]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 424
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXjAF0AadaQU",
        "colab_type": "code",
        "outputId": "26b68b54-bc75-41d4-9c3f-ae765abb72f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_layer = Embedding(7, 768, input_length=2)\n",
        "target = embedding_layer(target_input)\n",
        "context = embedding_layer(context_inputs)\n",
        "context.shape, target.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([12, 2, 768]), TensorShape([12, 768]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 432
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t71lMbdGd-xC",
        "colab_type": "code",
        "outputId": "30b05b8b-aa74-4efc-9da0-b124bc58f8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cbow = Lambda(lambda x: K.mean(x, axis=[1]))\n",
        "context = cbow(context)\n",
        "context.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([12, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 433
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S1CZHoJghvx",
        "colab_type": "code",
        "outputId": "e7119180-6791-4934-8585-1b77e5f3c048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dotted = dot([target, context], axes=1)\n",
        "dotted.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([12, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 435
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZF62FmYiUFb",
        "colab_type": "code",
        "outputId": "a55d42a3-f680-4b77-ad42-33b6ad1c9f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "outvec = Dense(1, activation='sigmoid')\n",
        "done = outvec(dotted)\n",
        "# done.shape\n",
        "tf.convert_to_tensor(labels[0]).shape, tf.squeeze(done).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([12]), TensorShape([12]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 437
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MVubitzliSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
